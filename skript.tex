\documentclass[a4paper,ngerman]{scrbook}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[ngerman]{babel}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{cancel}

%Compiler
\usepackage{ifxetex}
\usepackage{ifluatex}
\ifxetex
  \usepackage{fontspec,xunicode}
  \catcode`\ß=13
  \defß{\ss}
\else\ifluatex
  \usepackage{fontspec,xunicode}
\else
  \usepackage[utf8]{inputenc}
\fi\fi
% /Compiler

\newcommand{\bO}{\ensuremath{\mathcal{O}}}%
\newcommand{\R}{\ensuremath{\mathbb{R}}}%

\title{Computergestützte Methoden der exakten Naturwissenschaften }
\subtitle{Computerphysik}
\date{Wintersemester 2013/2014}
\author{Carlos Martín Nieto}
\begin{document}
\maketitle
\chapter{Fehler}

Ziel der Naturwissenschaften: Beschreibung der natur durch (einfache) Gleichungen und deren Lösungen

\paragraph{Problem}

Gleichungen typischerweise nicht lösbar mit Papier und Bleistift.
\begin{itemize}
\item[Lösung 1] Gleichungen vereinfachen $\hat{=}$ Näherung/Approximation
\item[Lösung 2] Numerische Lösung von Gleichungen $\implies$ \boxed{\text{diese Vorlesung}}
\end{itemize}

in Naturwissenschaften wichtig: Genauigkeit (den Fehler) von numerischen Rechenergebnissen beurteilen! Es gibt verschiedene Fehlerquellen
\begin{enumerate}
\item Eingabefehler durch Ungenauigkeiten in den Eingabedaten.
\item Näherungsfehler wenn statt exakten mathematische Ausdrücke vereinfachte Ausdrücke benutzt werden.
\item Modelfehler durch vereinfachte physikalische Modelle.
\item Rundungsfehler durch die numerische Darstellung von Zahlen und der damit verbundenden endlichen Genauigkeiten.
\end{enumerate}

\section{Näherungsfehler}

Viele mathemiatische Ausdrúcke, die in der Physik auftreten sind in der exakten Formulierung nicht oder sehr aufwendig zu berechnen $\to$ Approximation $\to$ Näherungsfehler. Häufig: Funktionen definiert durch unendliche Reihen.

\paragraph{Bsp 1}
Exponenzialfunktionen

Die Funktion ist $e^x = \displaystyle\sum^\infty_{n=0} \frac{x^n}{n!}$. Die Näherung $e^x = \displaystyle\sum^N_{n=0} \frac{x^n}{n!}$

$\displaystyle\frac{df(x)}{dx} = F(x)$ werden durch e-Funktion gelöst.

\paragraph{Bsp 2}
Lösung einer Differenzialgleichung im Kontinuum wird ersetzt durch Lösung der diskretisierten Gleichung.

Ausgangs-DG: $\displaystyle\frac{d}{dx}f(x) = a\cdot f(x)$, Lösung $f(x) = e^{ax}$.

Lösung durch Beschränkung auf diskrete Gitterpunkte. $x_i$ mit $x_{i+1} - x_i = \Delta x \to$

\[
f(x) = \frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i} = a\cdot \frac{f(x_{i+1}) - f(x_i)}{2}
\]

Verbesserung der Näherung durch Verfeinerung der Diskretisierung, also $\Delta x \to 0$.

Nachteil: mehr Rechenoperationen und damit mehr Rechenzeit, außerdem mehr Rundingsfehler (mehr in \autoref{sec:rundungsfehler}).

Ziel der Numerik: optimaler Kompromiss zwischen Fehler und Rechenzeit finden.
\section{Modellfehler}
\label{sec:modellfehler}

\paragraph{Beispiel}
Planetenbewegung

Erster Keppler Gesetzt: Planeten bewegen sich auf elyptischen Bahnen, in einem Brennpunkt steht die Sonne.

Neutonische Bewegungsgleichung
\[
\vec{F} = m\cdot \vec{a} = m\cdot \frac{d^2\vec{r}(t)}{dt^2} = -G \frac{Mm\vec{r}}{|r|^3}
\]

\paragraph{Modelnäherungen}

\begin{enumerate}
\item Sonnenmass $M \gg$ Planetenmasse $m$ (leicht korrigierbar)
\item Reibungskraft $\vec{F}_R = -\gamma \frac{d\vec{r}(t)}{dt}$ vernachlässigt. OK für Planeten, wichtig für kleine Objekte.
\item Gravitationsgesetzt in der einfachen Form gilt nur für Kugeln!
\item Relativistische Effekte $\to$ Merkur-Perikeldrehung.
\item Mehrkörperproblem $r_i(t)$, $i=1,\dots,N$
  \[
  m_i \frac{d^2r_i(t)}{dt^2} = \vec{F_i} = -\sum_{j\neq i} G m_i m_j \frac{\vec{r_i}(t) - \vec{r_j}(t)}{|v_i(t) - r_j(t)|^3}
  \]
\end{enumerate}

Gleichung für $N>2$ wechselwirkende Massen kann leicht hingeschrieben werden (durch Summatia der Knüpfe). Lösung aber nur nummerisch möglich! Annahme: $N=3$ Dreikörperproblem als 3-Stöße!

\chapter{Blah blah}

\chapter{Lineare Gleichungssyteme}
\section{Gauß}

Kompliziertes Zeug.

\section{LR-Zerlegung (LU in en)}

Möchte man mehrere LGS mit diasselben $A$ aber anderere rechter Seite lösen so empfiehlt es sich die Elementaren Umforomungen zu merken.

Für $A =
\begin{pmatrix}
  1 & 2 & 3\\
  6 & -2 & 2\\
  2 & 1 & -4
\end{pmatrix}
$ führen wir durch

\begin{align*}
z_2 = z_2 - 6z_1 &\to L_1 =
\begin{pmatrix}
  1 & 0 & 0\\
  -6 & 1 & 0\\
  0 & 0 & 0
\end{pmatrix}\\
  z_3 = z_3 + 3z_1 &\to L_2 =
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix}\\
  z_3 = z_3 + \frac{1}{2}z_2 &\to L_3 =
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}\\
L_1A &=
\begin{pmatrix}
  1 & 2 & 3\\
  0 & -14 & -16\\
  -3 & 1 & -4
\end{pmatrix}
\end{align*}

\begin{align*}
  L_2L_1 &=
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & 0 & 1
  \end{pmatrix} =
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix}\\
L &= L3(L_1\cdot L_1) = 
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix} =
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}\\
  LA &= L_3L_2L_1A - R\\
  R &= LA =
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 2 & 3\\
    6 & -2 & 2\\
    2 & 1 & -4
  \end{pmatrix} =
  \begin{pmatrix}
    1 & 2 & 3\\
    0 & -14 & -16\\
    0 & 0 & -3
  \end{pmatrix}
\end{align*}

Dies ist die rechts-obere Dreiechsmatrix aus der letzten VL.

\paragraph{Bsp}

Lösung von $A\vec{x} = \vec{c}$ mit $\vec{c} =
\begin{pmatrix}
  12\\ -16\\ 2
\end{pmatrix}$

\begin{align*}
  L\vec{c} &=
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}
  \begin{pmatrix}
    12\\ -16\\ 2
  \end{pmatrix}
  =
  \begin{pmatrix}
    12 \\ -88\\ -6
  \end{pmatrix}
\end{align*}

D.h\@. wir losen $R\vec{x} = y\cdot
\begin{pmatrix}
  1 & 2 & 3 & 2\\
  0 & -14 & -16 & -88 \\
  0 & 0 & -3 & -6
\end{pmatrix}$. Rückeinsetzen
\begin{align*}
  x_3 &= \frac{-6}{-3} = 2\\
  14x_2 &= 88 - 16x_3\\
  &= 88 - 32 = 56\\
  \implies x_2 &= 4\\
  x_1 &= 12 - 2x_2 - 3x_3\\
  &= 12 - 8 - 6 = -2\\
  \implies \vec{x} =
  \begin{pmatrix}
    -2\\ 4\\ 2
  \end{pmatrix}
\end{align*}

Satz. Für jede $n \times n$-Matrix, für die der Gauß-Algorithmus durchführbar ohne Zeilenaustauschung durchführbar ist, gibt es $n\times n$-Matrizen $L$ und $R$ mit der Eigenschaften:
\begin{itemize}
\item $L$ ist eine links-untere Dreiechsmatrix mit $l_{ii} = 1$ für $i=1,\dots,n$
\item $R$ ist eine rechts-obere Dreiechsmatrix mit $r_{ii} \neq 0$ für $i=1,\dots,n$.
\item $A = L^{-1}R$ bezeichnet man als \underline{$LR$-Zerlegung} von $A$.
\end{itemize}

Es gilt: \framebox{$A\vec{x} = \vec{b} \iff L\vec{b} = y$ und $R\vec{x} = \vec{y}$}

\section{Cholesky-Zerlegung}
\label{sec:chorlesky}

\paragraph{Definition}

Eine Symmetrische $n \times n$-Matrix $A$ heißt \underline{positiv-definit}, wenn für alle $\vec{x}~\in~R^n$, $\vec{x} = 0$ gilt: $\vec{x}^{T} A \vec{x} > 0$.

\paragraph{Satz}

Für jede positiv-definierte Matrix $A$ gibe es genau eine rechts-obere Dreiechsform mit $r_{ii} > 0$ für $i=1,\dots,n$ und $A=R^{T}R$. Diese Zerlegung heißt Cholesky-Zerlegung.

Zum berechnen der Cholesky-Zerlegung geht man wie folgt vor (Pseudo-Code)


\begin{verbatim}
for i = 1,...n,
  s = a_ii - \sum_{k=1}^{i-1} r_{ki}^2 // für i=1 ist s=a_11
  if s \leq 0
    stop // $A$ ist nicht positiv definit
  else
    r_ii = \sqrt{s}
    for j = i+1,...,n // nicht-Diagonalelemente
      r_{ij} = \frac{1}{r_ii} (a_ij - \sum_{k=1}^{i-1} r_{ki}r_{kj})
    endfor
endfor
\end{verbatim}

\paragraph{Bsp}

$A =
\begin{pmatrix}
  4 & 4 & 2\\
  4 & 5 & 5\\
  2 & 5 & 26
\end{pmatrix}
$
$
  R =
  \begin{pmatrix}
    r_{11} & r_{12} & r_{13}\\
    0 & r_{22} & r_{23}\\
    0 & 0 & r_{33}
  \end{pmatrix} =
  \begin{pmatrix}
    2 & 2 & 1\\
    0 & 1 & 3\\
    0 & 0 & 4
  \end{pmatrix}
$
\begin{itemize}
\item $i=1$

    $s = a_{11} = 4 > 0 \implies r_{11} = \sqrt{4} = 2$
    \begin{itemize}
    \item $j=2$

      $r_{12} = \frac{1}{2}(4) = 2$
    \item $j=3$
      $r_{13} = \frac{1}{2}(2 - 0) = 1$
    \end{itemize}
  \item $i=2$

    $s = a_{22} - \sum_{k=1}^{i-1} r_{ki}^2 = 5 - 2^2 =1 > 0 \implies r_{22} = \sqrt{1} = 1$
\begin{itemize}
\item $j=3$

  $r_{23} = \frac{1}{1} (5 - \sum_{k=1}^1 r_{12} r_{12}) = 5 - 2 = 3$
\end{itemize}
\item $i=3$

  $s = a_{33} = a_{33} - \sum_{k=1}^{2} r_{k3}^2 = 26 - (1^2 + 3^2) = 16 > 0 \implies r_{33} = \sqrt{16} = 4$
\end{itemize}

Test:

\begin{align*}
R^TR &=
\begin{pmatrix}
  2 & 0 & 0\\
  2 & 1 & 0\\
  1 & 3 & 4
\end{pmatrix}
\begin{pmatrix}
  2 & 2 & 1\\
  0 & 1 & 3\\
  0 & 0 & 4
\end{pmatrix} =
\begin{pmatrix}
  4 & 4 & 2\\
  4 & 5 & 5\\
  2 & 5 & 26
\end{pmatrix} = A\\
\intertext{aber:}
RR^T &= 
\begin{pmatrix}
  2 & 2 & 1\\
  0 & 1 & 3\\
  0 & 0 & 4
\end{pmatrix}
\begin{pmatrix}
  2 & 0 & 0\\
  2 & 1 & 0\\
  1 & 3 & 4
\end{pmatrix} =
\begin{pmatrix}
  9 & 5 & 4\\
  5 & 10 & 12\\
  4 & 12 & 16
\end{pmatrix} \neq A
\end{align*}

\paragraph{Bemerkung}

Der numerische Aufwand des Cholesky-Verfahrens beträgt $\left(\frac{1}{6} n^3 + \frac{1}{2}n^2 - \frac{2}{3} n\right)$ Punktoperationen und $n$ Wurzelberechnungen.
Das Gauß-Verfahren benötigt $\left(\frac{n^3}{3} + \frac{n}{3}\right)$ Operationen, also für $n \geq 2$ ist Cholesky schenller!

\section{Fehlerrechnung bei LGS}
\label{sec:fehlerlgs}

Wir wollen untersuchen, wie sich Fehler in einem LGS auf dessen Lösung auswirken. Dazu benötigen wir ein Maß.

\paragraph{Definition}

Eine Abbildung  $\| \cdot \|$ $\R^n \to \R$ heißt \underline{Vektornorm}, wenn für alle $\vec{x}, \vec{y} \in R^n$ und alle $\lambda \in \R$ gilt:
\begin{itemize}
\item $\|\vec{x}\| \geq 0$ und $\|\vec{x}\| = 0$ wenn $\vec{x} = 0$.
\item $\|\lambda x\| \geq 0$ und $\|\vec{x}\| = 0$ wenn $\vec{x} = 0$.
\item $\|\vec{x} + \vec{y}\| \leq \|\vec{x}\| + \|\vec{y}\|$
\end{itemize}

Beispiele sind

\begin{itemize}
\item Summenorm (1-Norm) $\|\vec{x}\| = \sum_{i=1}^n |x_i|$
  \item Maximumnorm ($\infty$-Norm) $\|\vec{x}\|_{\infty} = \max_{i=1,\dots,n} |x_i|$
\item euklidische Norm (2-Norm) $\|\vec{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}$
\end{itemize}

Alle Vektornormen sind äquivalent, d.h\@. es existieren $c_1,c_2 \in \R$ so dass

\[
c_1\|\vec{x}\|_a \leq \|\vec{x}\|_b \leq c_2 \|\vec{x}\|_a
\]

D.h\@. falls es eine Vektorfolge in einer Norm $a$ konvergiert, so konvergiert sie auch inder Norm $b$. Man kann entsprechende Matrixnomren definieren für eine $A = (a_{ij})$.
\begin{itemize}
\item Spaltensumme (1-Norm) $\|A\|_1 = \max_{j=1,\dots,n} \sum_{i=1}^n |a_{ij}|$
\item Spektralnorm (2-Norm) $\|A\|_2 = \sqrt{S(A^TA)}$, mit $S(A) = \max\{\lambda\}$ "`Spektralradius von $A$"', $\lambda\colon$ Eigenwerte von $A$.
\item Zeilensummennorm ($\infty$-Norm) $\|A\|_\infty = \max\limits_{i=1,\dots,n} \sum_{i=1}^n |a_{ij}|$
\end{itemize}

\paragraph{Bemerkung}

Matrixnormen erfüllen alle Eigenschaften der zugrundelegenden Vektornormen. Insbesondere auch die Äquivalentz

\[
\|A\vec{x}\|_v \leq \|A\|_v\|\vec{x}|_v
\]

Dann sagt man die Norm ist kompatibel

\paragraph{Satz}

Sei $\|\cdot\|$ eine Norm, $A$ eine reguläre $n\times n$-Matrix die Vektoren $\vec{x}, \vec{x'}, \vec{b}, \vec{b'} \in \R^n$ so dass $A\vec{x} = \vec{b} und A\vec{x'} = \vec{b'}$.

Dann gilt

\begin{align*}
  A\cdot (\vec{x}\vec{x'}) &= \vec{b} - \vec{b'}\\
  \vec{x} - \vec{x'} &= A^{-1}(\vec{b} - \vec{b'})\\
  \|\vec{x}-\vec{x'}\| \leq \|A^{-1}\| \|\vec{b} - \vec{b'}\|
\end{align*}

und entsprechend nach Multiplikation mit
\begin{align}
  \|A\|\|\vec{x}\| \geq \|\vec{b}\| \to   \|\frac{1}{\vec{x}}\| \leq     \frac{\|A\|}{\|\vec{b}\|}\\
  \frac{\|\vec{x} - \vec{x'}\|}{\|\vec{x}\|} \leq \|A\| \|A^{-1}\| \cdot \frac{\|\vec{b}-\vec{b'}\|}{\|\vec{b}\|}
\end{align}

Dann nennt $\|A\| \|A^{-1}\| = \text{cond}(A)$ die \underline{Konditionszahl} der Matrix $A$ bzgl\@. der verwendeten Norm.

\paragraph{Bemerkung}

Die Konditionszahl gibt also die max\@. Verstärkung des realtiven Fehlers (3.2) an, wahrend $\|A^{-2}\|$ die maximale Verstärkung des absoluten Fehlers ist (3.1).

\paragraph{Beispiel}

$A\vec{x} = \vec{b}$ mit $\vec{A} =
\begin{pmatrix}
  3 & -6 \\ -6 & 8
\end{pmatrix}$ und $ \vec{b} =
\begin{pmatrix}
  18\\ 28
\end{pmatrix}
$

\begin{align*}
  \det A &= 24 - 36 = -12\\
  A^{-1} = \frac{1}{\det A} \cdot \text{adj}(A) = \frac{1}{-12}
  \begin{pmatrix}
    8 & -6\\ -6 & 3
  \end{pmatrix} =
  \begin{pmatrix}
    \frac{-2}{3} & \frac{1}{2}\\ \frac{1}{2} & \frac{-1}{4}
  \end{pmatrix}\\
  \|A\|_1 &= \max\{9,14\} = 14 = \|A\|_\infty\\
  \|A^{-1}\| & =\max\left\{\frac{7}{6}, \frac{3}{4}\right\} = \frac{7}{6} = \|A^{-1}\|_\infty\\
\|\vec{b}\|_1 &= 46 & \text{cond} A = \|A\|_1 \|A^{-1}\|
\end{align*}

Wir nehmen an $\|\vec{b}-\vec{b'}\| \leq 0.1$.

Abschätzung absoluter Fehler: $|\vec{x}\vec{x'}| \leq \|A^{-1}\| \|\vec{b} - \vec{b'}\| \leq \frac{7}{6}\cdot 0.1 = 0.11666\dots$.

Relativer Fehler $\frac{\|\vec{x}-\vec{x'}\|}{\|\vec{x}\|} \leq \text{cond} A \frac{\|\vec{b}-\vec{b'}}{\|\vec{b}\|} \leq 16.333\dots  \cdot \frac{0.1}{16} \approx 3.35\%$

Der relativer Fehler hängt ab von cond($A$) und $\frac{1}{\|\vec{b}\|}$!

\paragraph{Satz}

Ist nicht nur die rechte Seite eines LGS fehlerbehaftet, sonder auch die Koeffizientenmatrix, so gilt fúr die Lösung der beiden LGS $A\vec{x} = \vec{b}$ und $A'\vec{x'} = \vec{b'}$ mit $\Delta A = A- A'$ und $\Delta \vec{x} = \vec{x} - \vec{x'}$, $\Delta\vec{b} = \vec{b}-\vec{b'}$

\[
\frac{\|\Delta\vec{x}\|}{\|\Delta\vec{x}\|} \leq \frac{\text{cond}(A)}{1-\text{cond}(A) \frac{\|\Delta A\|}{\|A\|}} \left(\frac{\|\Delta A\|}{\|A\|} + \frac{\|\Delta \vec{b} \|}{\|\vec{b}\|}\right)
\]

\section{Iterative Verfarhen}
\label{sec:iterative-verfahren}

Idee: LGS $Ax = b$ ist äquivalent zu einem vektoriellen Nullstellenproblem $Ax - b = \vec{0}$. Anstelle des Nullstellenrproblems betrachtet man das Fixpunktproblem!

Zerlegung der Matrix $A = I + A - I$ ($I$ ist die Einheitsmatrix). Aus $Ax - b = \vec{0}$ wird


\begin{align*}
  \vec{0} &= (I + A - I)x - b \to Ix = (I - A)x + b\\
  x &= (I-A)x + b
\end{align*}

vektorielle Fixpunktgleichung, kann als Iterationsgleichung interpretiert werden. Startwert $x^{(0)} \to x^{(1)} = (I-A)x^{(0)} + b$ und allgemein

\[
x^{(n+1)} = (I-A)x^{(n)} + b
\]

Fixpunkt der Iterationsgleichung $\hat{=}$ Lösung des LGS.

\paragraph{Beispiel}

$A =
\begin{pmatrix}
  4 & -1 & 1\\
  -2 & 5 & 1\\
  1 & -1 & 5
\end{pmatrix}
$, $b =
\begin{pmatrix}
  5\\ 11\\ 12
\end{pmatrix}
$


\begin{align*}
  x^{(n+1)} &= \left[
    \begin{pmatrix}
      1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1
    \end{pmatrix}
    -
    \begin{pmatrix}
      4 & -1 & 1\\ -2 & 5 & 1\\ 1 & -1 & 5
    \end{pmatrix}
  \right]x^{(n)} +
  \begin{pmatrix}
    5 \\ 11\\ 12
  \end{pmatrix}\\
  x^{(n+1)} &=
  \begin{pmatrix}
    -3 & 1 & -1\\ 2 & -4 & -1\\ -1 & 2 & -4
  \end{pmatrix} x^{(n)} +
  \begin{pmatrix}
    5 \\ 11\\ 12
  \end{pmatrix}
\end{align*}


\begin{center}
  \begin{tabular}{c|c|c|c|c|}
    i & 0 & 1 & 2 & 3\\\hline
    \multirow{3}{*}{$x^{(i)}$} & 0 & 5 & -11 & 22\\\cline{2-5}
    & 0 & 11 & -35 & 148\\\cline{2-5}
    & 0 & 12 & -19 & 29
  \end{tabular}\\
\end{center}
Ansatz nicht konvergent! Betracthe alternive additive Zerlegung $A = A_1 + A_2$

\subsection{Jacobi Verfahren (Gesamtschritt)}
\label{sec:jacobi-gesamt}

Betrachte Zerlegung


\begin{align*}
  A &= L
  \begin{pmatrix}
    0 & 0 & 0 & \cdot\\
    a_{21} & 0 & 0 & \cdot\\
    a_{31} & a_{32} & 0 & \cdot\\
    \cdot & \cdot & \cdot & \cdot
  \end{pmatrix}
  + D
  \begin{pmatrix}
    a_11 & 0 & 0 & \cdot\\
    0 & a_{22} & 0 & \cdot\\
    0 & 0 & a_{33} & \cdot\\
    \cdot &\cdot &\cdot &\cdot
  \end{pmatrix}
  + R
  \begin{pmatrix}
    0 & a_{12} & a_{13} & \cdot\\
    0 & 0 & a_{23} & \cdot\\
    0 & 0 & 0 & \cdot\\
    \cdot &\cdot &\cdot &\cdot
  \end{pmatrix}\\
  Ax-b = 0 &= (L + D + R)x-b \to Dx = -(L+R)x+b\\
  x &= \{\text{FP-Gl}\}-D^{-1}(L+R)x + D^{-1}b \to \{\text{It-Gl}\}x^{(n+1)} = -D(L+R)x^{(n)}  D^{-1} b
\end{align*}

Bsp

\[
A =
\begin{pmatrix}
  4 & -1 & 1\\ -2 & 5 & 1\\ 1 & -2 & 5
\end{pmatrix}
 \to L =
 \begin{pmatrix}
   0  & 0 & 0\\ -2 & 0 & 0\\ 1 & -2 & 0
 \end{pmatrix}
 ,\ D=
 \begin{pmatrix}
   4 & 0 & 0\\ 0 & 5 & 0\\ 0 & 0 & 5
 \end{pmatrix}
 ,\ R=
 \begin{pmatrix}
   0 & -1 & -1\\ 0 & 0 & 1\\ 0 & 0 & 0
 \end{pmatrix}
\]


\begin{align*}
  x^{(n+1)} &= -
  \begin{pmatrix}
    0.25 & 0 & 0\\ 0 & 0.2 & 0\\ 0 & 0 & 0.2
  \end{pmatrix}
  \left[
    \begin{pmatrix}
      0 & -1 & 1\\ -2 & 0 & 1\\ 1 & -2 & 0
    \end{pmatrix} -
    \begin{pmatrix}
      5\\ 11\\ 12
    \end{pmatrix}
  \right]\\
  &=
  \begin{pmatrix}
    0 & 0.25 & -0.25\\ 0.4 & 0 & -0.2\\ -0.2 & 0.4 & 0
  \end{pmatrix} x^{(n)} +
  \begin{pmatrix}
    1.25\\ 2.2\\ 2.4
  \end{pmatrix}
\end{align*}

\begin{tabular}{c|c|c|c|c|c|c|}
  i & 0 & 1 & 2 & 3 & 4 & 5\\\hline
  \multirow{3}{*}{$x^{(i)}$} & 0 & 1.25 & 1.2 & 1.0475 & 1.0065 & 0.9973\\\cline{2-7}
  & 0 & 2.2 & 2.22 & 2.74 & 2.0094 & 1.9986\\\cline{2-7}
  & 0 & 2.4 & 3.03 & 3.048 & 3.0201 & 3.0024
\end{tabular}

\subsection{Einzelschrittverfahren (Gauß-Seidel Verfahren)}
\label{sec:gauss-seidel}

Jakobi-Verfarhen in Komponenten:

%% FIMXE: function numbering per subsection?

\begin{align}
  x_1^{(n+1)} &= 0.25\cdot x_2^{(n)} - 0.25 \cdot x_3^{(n)} + 1.25\\
  x_2^{(n+1)} &= 0.4\cdot x_1^{(n)} - 0.2\cdot x_3^{(n)} + 2.2\\
  x_3^{(n+1)} &= -0.2\cdot x_1^{(n)} + 0.4 \cdot x_2^{(n)} + 2.4
\end{align}

Wenn man annimmt, dass $x^{(n+1)}$ komponentenweise näher am Lösungsvektor liegt, sollte man in Gl (2)(3.4) $x_1^{(n+1)}$ benutzen und in Gl (3)(3.5) $x_1^{(n+1)}$ und $x_2^{(n+1)}$, wie man hierunter sieht.

\begin{align}
  x_1^{(n+1)} &= 0.25\cdot x_2^{(n)} - 0.25 \cdot x_3^{(n)} + 1.25\\
  x_2^{(n+1)} &= 0.4\cdot x_1^{(n+1)} - 0.2\cdot x_3^{(n)} + 2.2\\
  x_3^{(n+1)} &= -0.2\cdot x_1^{(n+1)} + 0.4 \cdot x_2^{(n+1)} + 2.4
\end{align}

In Matrix-Schreibweise

\begin{align*}
  x^{(n+1)} &=
  \begin{pmatrix}
    0 & 0.25 & -0.25\\ 0 & 0 & -0.2\\ 0 & 0 & 0
  \end{pmatrix} x^{(n)} +
  \begin{pmatrix}
    0 & 0 & 0\\ 0.4 & 0 & 0\\ -0.2 & 0.4 & 0
  \end{pmatrix} +
  \begin{pmatrix}
    1.25\\ 2.2\\ 2.4
  \end{pmatrix}
  \intertext{oder:}
  x^{(n+1)} &= D^{-1} \left[ Rx^{(n)} +  Lx^{(n+1)} -b \right]\\
  Dx^{(n+1)} &= -Rx^{(n)} - Lx^{(n+1)} + b & \text{It.-Gl. Gauß-Seidel}\\
  \to (D+L) x^{(n+1)} &= - Rx^{(n)} + b\\
  \to x^{(n+1)} &= -(D+L)^{-1} Rx^{(n)} + (D+L)^{-1}b % box
\end{align*}

\paragraph{In der Praxis}

wird nicht das Inverse der Matrix $(D+L)$ berechnet, sondern das Gl.-Syst\@. $(D+L)x^{(n+1)} = -Rx^{(n)}+ b$ wird gelöst (durch Vorwärtseinsetzen)!

Zurück zum Beispiel

\begin{center}
  \begin{tabular}{c|c|c|c|c|c|}
    & 0 & 1 & 2 & 3 & 4\\\hline
    & 0 & 1.25 & 1.1175 & 1.006 & 1.001\\\hline
    & 0 & 2.7 & 2.001 & 2.007 & 2.0002\\\hline
    & 0 & 3.23 & 2.977 & 3.002 & 2.9998
  \end{tabular}
\end{center}
Konvergiert (etwas) schneller als Gesamtschrittverfahren.

\subsection{Konvergenz von linearen Iterationverfahren}

Gegeben sei eine lineare Matrix-Fixpunkt-Iterationsgleichung $x^{(n+1)} = Bx^{(n)}$ wobei $B$ eine $m \times m$-Matrix, $b \in \R^n$. $\overline{x}$ sei ein Fixpunt mit $\overline{x} = B\overline{x}  b$.

Sei $\|\cdot\|$ ine Matrixnorm (siehe letzte VL), dann gilt: \begin{itemize}
\item[] $\overline{x}$ ist ein anziehenden Fixpunkt, falls $\|B\| < 1$
\item[] $\overline{x}$ ist ein abstoßenden Fixpunkt, falls $\|B\| > 1$
\end{itemize}

Für anziehenden Fixpunkt: Fixpunktiteration konvergiert für \underline{alle} Startwerte.

\paragraph{Bemerkungen}

\begin{itemize}
\item für Gesamtschrittverfahren (Jakobi) gilt $B = -D^{-1}(L+R)$, für die $\infty$-Norm (Zeilensummennorm) gilt (im Hinterkopft behalten, dass $A = D+L+R$):
\begin{align*}
  \|B\|_\infty &= \max\limits_{i=1,m} \sum_{j\neq i} \frac{|a_{ij}|}{|a_ii|} = \max\limits_{i=1,m} \underbrace{\frac{1}{|a_{ii}|}}_{\text{aus $D^{-1}$}} \sum_{j\neq i} \underbrace{|a_{ij}|}_{\text{aus $(L+R)$}} < 1\\
&\iff |a_{ii}| > \sum_{j\neq i} |a_{ij}| \text{ für \underline{alle} $i$}\\
&\hat{=} \text{ "`Zeilensummenkriterium"'}
\end{align*}

So eine Matrix heißt "`diagonaldominant"'

\item Im Einzelschrittverfahren ist $B = -(D+L)^{-1} R$ und man kann zeigen, dass $\| (D+L)^{-1} R \|_\infty \leq \|D^{-1} (L+R)\|_\infty$. \underline{Also}: konvergiert das Gesamtschrittverfahren, so konvergiert auch das Einzelschrittverfahren (Inverse ist nicht immer richtig).
\end{itemize}

\section{Nichtlineare Gleichungsssyteme | Newton Verfahren}
\label{sec:nichtlineare}

Häufig: $n$ nicht-lineare Gleichungen mit $n$ Unbekannten.\\
Gegeben: vektorielle Funktion $f\colon \R^n \to \R^n$.\\
Gesucht: Vektor $\overline{x} \in \R^n$ mit $f(\overline{x}) = \vec{0}$.

\begin{align*}
  f(x) &= f(x_1,\dots,x_n) =
  \begin{pmatrix}
    f_1(x_1,\dots,x_n)\\ f_2(x_1,\dots,x_n)\\ \vdots \\ f_3(x_1,\dots,x_n)
  \end{pmatrix} =
  \begin{pmatrix}
    0\\ 0\\ \vdots\\ 0
  \end{pmatrix}
\end{align*}

Es gibt kein einfaches Verfarhen, um zu prüfen, ob Gl.-Syst\@. lösbar ist! Wie viele Lösungen? $\to$ verallgemenere Newton-Verfahren auf $n$ Dimensionen!

\begin{align*}
  g\colon \R \to \R &\implies g(x) \tilde{=} g(x_0) + (x-x_0)g'(x_0)
  \overset{!}{=} 0\\
  f\colon \R^n \to \R^n &\implies f(x) \tilde{=} f\left(x^{(0)}\right) + D\left[ f\left(x^{(0)}\right)\right] \left(x-x^{(0)}\right)\overset{!}{=} 0
\end{align*}

Lösen des linearen Gl.-Syst\@.

\begin{align*}
  x &= x^{(0)} - \left[ D\left[f\left(x^{(0)}\right)\right]\right]^{-1} f\left(x^{(0)}\right)
\intertext{oder als Iterationsgleichung}
x^{(n+1)} &= x^{(n)} - \left[ D\left[f\left(x^{(n)}\right)\right]\right]^{-1} f\left(x^{(n)}\right)
\end{align*}

Noch mal: nie die inverse einer Matrix berechnen!

%% 20131107

Die Jacobi Matrix der partiellen Ableitungen:

\[
D\left[f(x)\right] = \frac{\partial f_i(x)}{\partial x_j} =
\begin{pmatrix}
  \frac{\partial f_i}{\partial x_1} & \frac{\partial f_i}{\partial x_2} & \dots & \frac{\partial f_i}{\partial x_n}\\
  \vdots \\
  \frac{\partial f_n}{\partial x_1} & & \dots & \frac{\partial f_n}{\partial x_n}
\end{pmatrix}
\]

\paragraph{Bsp}

\begin{align*}
  f(x_1,x_2) =
  \begin{pmatrix}
    2x_1 + 4x_2\\ 4x_1 + 8x^3_2
  \end{pmatrix} \to D[f] =
  \begin{pmatrix}
    2 & 4\\ 4 & 24x^2_2
  \end{pmatrix}
\end{align*}

wähle als Startwert
\begin{align*}
  x^{(0)} &=
  \begin{pmatrix}
    4\\ 2
  \end{pmatrix} \to D[f(x_0)] =
  \begin{pmatrix}
    2 & 4\\ 4 & 96
  \end{pmatrix}\\
    f(x^{(0)}) &=
    \begin{pmatrix}
      16\\ 80
    \end{pmatrix}\\
    D[f(x^{(0)}](x-x^{(0)}) &\overset{!}{=} -f(x^{(0)})\\
    \begin{pmatrix}
      2 & 4\\ 4 & 96
    \end{pmatrix} (x-x^{(0)}) &=
    \begin{pmatrix}
      -16\\ -80
    \end{pmatrix}
\end{align*}

\begin{tabular}{c|c|c|c|c}
  & $n=1$ & 2 & 3 & 4\\\hline
  \multirow{2}{*}{$x^{(n)}$} & -2.91 & -2.30 & -2.-5 & -2.008\\
  & 1.45 & 1.15 & 1.03 & 1.009
\end{tabular}

Iteration konvergiert schnell gegen $\overline{x} =
\begin{pmatrix}
  -2 \\ 1
\end{pmatrix}
$, eine von 3 Lösungen.

\paragraph{Satz}

Das Newton-Vefahren konvergiert quadratisch, wenn:
\begin{itemize}
\item $x^{(0)}$ nahe genug an $\overline{x}$ liegt
\item $D\left[f(\overline{x})\right]$ regulär ist
\item $f$ dreimal stetig differenzierbar ist
\end{itemize}

\paragraph{Achtung}

Nicht immer konvergiert das Newton-Verfahren gegen einie Nullstelle. Bsp:

\[
f =
\begin{pmatrix}
  x_1^3-x_2-1\\ x_1^2-x_2
\end{pmatrix}
\]

Startwert $x^{(0)} =
\begin{pmatrix}
  -1\\ 0
\end{pmatrix}
$ konvergiert gegen $\overline{x} =
\begin{pmatrix}
  0\\ -0.5
\end{pmatrix}
$ aber $f(\overline{x}) =
\begin{pmatrix}
  -0.5\\ 0.5
\end{pmatrix}
$! Grund:

\begin{align*}
  D[f] &=
  \begin{pmatrix}
    3x_1^2-1\\ 2x_1-1
  \end{pmatrix}
  \intertext{und}
  D[f(\overline{x})] &=
  \begin{pmatrix}
    0 & -1\\ 0 & -1
  \end{pmatrix}
\end{align*}

$\implies$ Lösung der Iterations-Gleichung checken auf $f(\overline{x}) = 0$.

\paragraph{Bemerkung}

in der Praxis werden partielle Ableitungen durch differenzen angenährt:

\[
D[f(x)]_{ij} = \frac{f_i(x + he_j) - f_i(x)}{h}
\]

mit $e_j$ Einheitsvektor in $j$-Richtung.

\subsection{Vereinfachtes Newton-Verfahen}
\label{sec:vereinnewt}

Rechenaufwan pro Schritt minimiert, wenn $D[f(x^{(0)}]$ benutzt wird. Konvergenz nur noch linear!

\chapter{Interpolation \& Ausgleichsrechnung ("`Fitten"')}
\label{chap:fitten}

In vielen Anwendungen: Messdaten sollen durch eine Formel beschrieen werden, zum Beispiel um Integrale, Differentiale etc\@. zu berechnen.

\paragraph{Def}

gegeben $n+1$ Wertepaare $(x_i,f_i)$ mit $i=0,\dots,n$; gesucht stetige Funktion $f(x)$ mit $f(x_i) = f_i$ für \underline{alle} $i=0,\dots,n$.

\begin{itemize}
\item $\{x_n\}$ Stützstellen
\item $\{f_n\}$ Stützwerte
\item $\{x_n,f_n\}$ Stützpunkte
\item $f(x)$ ist die Interpolierende der Stützpunkte
\end{itemize}
\paragraph{Bemerkung}

Interpolationsproblem ist nicht eindeutig!


\section{Polynom-Interpolation}
\label{sec:interpo}

Polynom $n$-Ren Grades $\to n+1$ Freiheitsgrade, es ist also möglich die $n+1$ Koeffizienten so zu wählen, dass $f(x_i) = f_i$ ist!

\paragraph{Satz}

gegeben $n+1$ Wertepaare $(x_i,f_i)$. Es gibt genau ein Polynom vom Grad höchstens $n$, so dass $p(x_i) = f_i$ für alle $i$.

$n = 1$: lineare Interpolation, $n=2$: quadratische Interpolation, usw.

\paragraph{Ineffiziente Methode}

explizite Lösung eines linearen Gl.-Syst\@. $p(x)= \sum_{i=0}^3 a_i x^i$, $f_i = \sum_{i=0}^3 a_i x_j^i$

\begin{tabular}{c||c|c|c|c}
  $x_i$ & -1 & 0 & 1 & 2\\\hline
  $f_i$ & 5 & -2 & 9 & -4
\end{tabular}

4 Gleichungen für 4 Unbekannte:

\begin{align*}
  5 &= a_0 + a_i(-1) + a_2(-1)^2 + a_3(-1)^3\\
  -2 &= a_0 + \cancel{a_i\cdot(0)}, \dots\\
  9 &= a_0 + a_i(1) + a_2(1)^2 +a_3(1)^3\\
  -4 &= a_0 + a_i(2) = a_2(2)^2 + a_4(2)^3
\end{align*}

Als matrizen

\begin{align*}
  \begin{pmatrix}
    5\\-2\\9\\=4
  \end{pmatrix} &=
  \begin{pmatrix}
    1 & -1 & 1 & -1\\
    1 & 0 & 0 & 0\\
    1 & 1 & 1 & 1\\
    1 & 2 & 4 & 8
  \end{pmatrix}
  \begin{pmatrix}
    a_0\\ a_1\\a_2\\a_3
  \end{pmatrix}
\end{align*}

Lösung (siehe VL): $a_0 = -2$, $a_1=9$, $a_2=9$, $a_3=-7$, $p(x) = -2+9x+9x^2-7x^3$

[Graph davon]

\paragraph{Lagrange Form des Interpolationspolynoms}

\begin{align*}
  p(x) &= \sum_{i=0}^n f_i l_i(x)
  \intertext{mit}
  l_i(x) &= \prod_{\substack{j=0\\j\neq i}}^n \frac{x - x_j}{x_i-x_j}
\end{align*}

in unserem Beispiel

\begin{align*}
  l_o(x) &= \left(\frac{x-x_1}{x_0 - x_1}\right)\left(\frac{x-x_2}{x_0 - x_2}\right)\left(\frac{x-x_3}{x_0 - x_3}\right)
\end{align*}

$l_0(x)$ ist Null an allen Stützstellen außer an $x_0$, und $l_0(x_0) = 1$. (analog für alle anderen Polynome $l_1,l_2,l_3$). $\to p(x_i) = f_i$!!

\begin{align*}
  l_o(x) &= \left(\frac{x-0}{-1 - 0}\right)\left(\frac{x-1}{-1 - 1}\right)\left(\frac{x-2}{-1 -2}\right) = \frac{-x(x-1)(x-2)}{6}
\end{align*}

jedes $l_i(\star)$ ist ein Polynom dritten Grades $\to p(x_0)$ auch dritten Grad. Berechnung von $p(x)$ durch Summation von $l_i(x)$ immer noch zu aufwändig $\hat{=}$ es gibt schnellere Methode!

\paragraph{Newtonsche Interpolationaformel}

Gegeben Wertpaare $(x_i,f_i)$ für $i=0,\dots,n$. Berechnung von "`dividierende Differenzen"':

\begin{itemize}
\item[] für $k=1\dots n$
  \begin{itemize}
  \item[] für $i=0\dots n-k$
    \begin{itemize}
    \item[] $f(x_i,x_{i+1}, \dots, x_{i+k}) = \frac{f(x_i+1, \dots, x_{i+k}) - f(x_i \dots x_{i+k-1})}{x_{i+k} - x_i}$
    \end{itemize}
  \item[] ende für
  \end{itemize}
\item[] ende für
\end{itemize}

im Schema für $n=2$.

\begin{tabular}{cccccccc}
  $x_i$ & $f_i$  & $f(x_i)$\\
  $x_0$ & $f(x_0)$\\
  & & $f(x_0,x_1) = \frac{f(x_1) - f(x_0)}{x_1 - x_0}$\\
  $x_1$ & $f(x_1)$ & & $f(x_0, x_1, x_2) = \frac{f(x_2,x_1) - f(x_0,x_1)}{x_2-x_0}$\\
  & & $f(x_1,x_2) = \frac{f(x_2) - f(x_1)}{x_2 - x_1}$\\
  $x_2$ & $f(x_2)$
\end{tabular}

\paragraph{Interpolationsformel}

\[
p(x) = \sum_{i=0}^n f(x_0,\dots,x_i)\prod_{j=0}^{i-1} (x-x_j)
\]

\end{document}
