\documentclass[a4paper,ngerman]{scrbook}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[ngerman]{babel}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{cancel}
\usepackage{gnuplottex}
\usepackage{dsfont}
\usepackage{siunitx}

%Compiler
\usepackage{ifxetex}
\usepackage{ifluatex}
\ifxetex
  \usepackage{fontspec,xunicode}
  \catcode`\ß=13
  \defß{\ss}
\else\ifluatex
  \usepackage{fontspec,xunicode}
\else
  \usepackage[utf8]{inputenc}
\fi\fi
% /Compiler

\newcommand{\bO}{\ensuremath{\mathcal{O}}}%
\newcommand{\R}{\ensuremath{\mathds{R}}}%
\newcommand{\rharp}[1]{\ensuremath{\overset{\rightharpoonup}{#1}}}%
\newcommand{\un}[2]{\ensuremath{#1^{(#2)}}}%
\newcommand{\rd}{\ensuremath{\text{rd}}}%

\title{Computergestützte Methoden der exakten Naturwissenschaften }
\subtitle{Computerphysik}
\date{Wintersemester 2013/2014}
\author{Carlos Martín Nieto}
\begin{document}
\maketitle
\chapter{Fehler}

Ziel der Naturwissenschaften: Beschreibung der natur durch (einfache) Gleichungen und deren Lösungen

\paragraph{Problem}

Gleichungen typischerweise nicht lösbar mit Papier und Bleistift.
\begin{itemize}
\item[Lösung 1] Gleichungen vereinfachen $\hat{=}$ Näherung/Approximation
\item[Lösung 2] Numerische Lösung von Gleichungen $\implies$ \boxed{\text{diese Vorlesung}}
\end{itemize}

in Naturwissenschaften wichtig: Genauigkeit (den Fehler) von numerischen Rechenergebnissen beurteilen! Es gibt verschiedene Fehlerquellen
\begin{enumerate}
\item Eingabefehler durch Ungenauigkeiten in den Eingabedaten.
\item Näherungsfehler wenn statt exakten mathematische Ausdrücke vereinfachte Ausdrücke benutzt werden.
\item Modelfehler durch vereinfachte physikalische Modelle.
\item Rundungsfehler durch die numerische Darstellung von Zahlen und der damit verbundenden endlichen Genauigkeiten.
\end{enumerate}

\section{Näherungsfehler}

Viele mathemiatische Ausdrúcke, die in der Physik auftreten sind in der exakten Formulierung nicht oder sehr aufwendig zu berechnen $\to$ Approximation $\to$ Näherungsfehler. Häufig: Funktionen definiert durch unendliche Reihen.

\paragraph{Bsp 1}
Exponenzialfunktionen

Die Funktion ist $e^x = \displaystyle\sum^\infty_{n=0} \frac{x^n}{n!}$. Die Näherung $e^x = \displaystyle\sum^N_{n=0} \frac{x^n}{n!}$

$\displaystyle\frac{df(x)}{dx} = F(x)$ werden durch e-Funktion gelöst.

\paragraph{Bsp 2}
Lösung einer Differenzialgleichung im Kontinuum wird ersetzt durch Lösung der diskretisierten Gleichung.

Ausgangs-DG: $\displaystyle\frac{d}{dx}f(x) = a\cdot f(x)$, Lösung $f(x) = e^{ax}$.

Lösung durch Beschränkung auf diskrete Gitterpunkte. $x_i$ mit $x_{i+1} - x_i = \Delta x \to$

\[
f(x) = \frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i} = a\cdot \frac{f(x_{i+1}) - f(x_i)}{2}
\]

Verbesserung der Näherung durch Verfeinerung der Diskretisierung, also $\Delta x \to 0$.

Nachteil: mehr Rechenoperationen und damit mehr Rechenzeit, außerdem mehr Rundingsfehler (mehr in \autoref{sec:rundungsfehler}).

Ziel der Numerik: optimaler Kompromiss zwischen Fehler und Rechenzeit finden.
\section{Modellfehler}
\label{sec:modellfehler}

\paragraph{Beispiel}
Planetenbewegung

Erster Keppler Gesetzt: Planeten bewegen sich auf elyptischen Bahnen, in einem Brennpunkt steht die Sonne.

Neutonische Bewegungsgleichung
\[
\vec{F} = m\cdot \vec{a} = m\cdot \frac{d^2\vec{r}(t)}{dt^2} = -G \frac{Mm\vec{r}}{|r|^3}
\]

\paragraph{Modelnäherungen}

\begin{enumerate}
\item Sonnenmass $M \gg$ Planetenmasse $m$ (leicht korrigierbar)
\item Reibungskraft $\vec{F}_R = -\gamma \frac{d\vec{r}(t)}{dt}$ vernachlässigt. OK für Planeten, wichtig für kleine Objekte.
\item Gravitationsgesetzt in der einfachen Form gilt nur für Kugeln!
\item Relativistische Effekte $\to$ Merkur-Perikeldrehung.
\item Mehrkörperproblem $r_i(t)$, $i=1,\dots,N$
  \[
  m_i \frac{d^2r_i(t)}{dt^2} = \vec{F_i} = -\sum_{j\neq i} G m_i m_j \frac{\vec{r_i}(t) - \vec{r_j}(t)}{|v_i(t) - r_j(t)|^3}
  \]
\end{enumerate}

Gleichung für $N>2$ wechselwirkende Massen kann leicht hingeschrieben werden (durch Summatia der Knüpfe). Lösung aber nur nummerisch möglich! Annahme: $N=3$ Dreikörperproblem als 3-Stöße!

\chapter{Blah blah}

\chapter{Lineare Gleichungssyteme}
\section{Gauß}

Kompliziertes Zeug.

\section{LR-Zerlegung (LU in en)}

Möchte man mehrere LGS mit diasselben $A$ aber anderere rechter Seite lösen so empfiehlt es sich die Elementaren Umforomungen zu merken.

Für $A =
\begin{pmatrix}
  1 & 2 & 3\\
  6 & -2 & 2\\
  -3 & 1 & -4
\end{pmatrix}
$ führen wir durch

\begin{align*}
z_2 = z_2 - 6z_1 &\to L_1 =
\begin{pmatrix}
  1 & 0 & 0\\
  -6 & 1 & 0\\
  0 & 0 & 0
\end{pmatrix}\\
  z_3 = z_3 + 3z_1 &\to L_2 =
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix}\\
  z_3 = z_3 + \frac{1}{2}z_2 &\to L_3 =
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}\\
L_1A &=
\begin{pmatrix}
  1 & 2 & 3\\
  0 & -14 & -16\\
  -3 & 1 & -4
\end{pmatrix}
\end{align*}

\begin{align*}
  L_2L_1 &=
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & 0 & 1
  \end{pmatrix} =
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix}\\
L &= L_3(L_2\cdot L_1) = 
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix} =
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}\\
  LA &= L_3L_2L_1A = R\\
  R &= LA =
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 2 & 3\\
    6 & -2 & 2\\
    2 & 1 & -4
  \end{pmatrix} =
  \begin{pmatrix}
    1 & 2 & 3\\
    0 & -14 & -16\\
    0 & 0 & -3
  \end{pmatrix}
\end{align*}

Dies ist die rechts-obere Dreiechsmatrix aus der letzten VL.

\paragraph{Bsp}

Lösung von $A\vec{x} = \vec{c}$ mit $\vec{c} =
\begin{pmatrix}
  12\\ -16\\ 2
\end{pmatrix}$

\begin{align*}
  L\vec{c} &=
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}
  \begin{pmatrix}
    12\\ -16\\ 2
  \end{pmatrix}
  =
  \begin{pmatrix}
    12 \\ -88\\ -6
  \end{pmatrix}
\end{align*}

D.h\@. wir losen $R\vec{x} = y\cdot
\begin{pmatrix}
  1 & 2 & 3 & 12\\
  0 & -14 & -16 & -88 \\
  0 & 0 & -3 & -6
\end{pmatrix}$. Rückeinsetzen
\begin{align*}
  x_3 &= \frac{-6}{-3} = 2\\
  14x_2 &= 88 - 16x_3\\
  &= 88 - 32 = 56\\
  \implies x_2 &= 4\\
  x_1 &= 12 - 2x_2 - 3x_3\\
  &= 12 - 8 - 6 = -2\\
  \implies \vec{x} =
  \begin{pmatrix}
    -2\\ 4\\ 2
  \end{pmatrix}
\end{align*}

Satz. Für jede $n \times n$-Matrix, für die der Gauß-Algorithmus durchführbar ohne Zeilenaustauschung durchführbar ist, gibt es $n\times n$-Matrizen $L$ und $R$ mit der Eigenschaften:
\begin{itemize}
\item $L$ ist eine links-untere Dreiechsmatrix mit $l_{ii} = 1$ für $i=1,\dots,n$
\item $R$ ist eine rechts-obere Dreiechsmatrix mit $r_{ii} \neq 0$ für $i=1,\dots,n$.
\item $A = L^{-1}R$ bezeichnet man als \underline{$LR$-Zerlegung} von $A$.
\end{itemize}

Es gilt: \framebox{$A\vec{x} = \vec{b} \iff L\vec{b} = y$ und $R\vec{x} = \vec{y}$}

\section{Cholesky-Zerlegung}
\label{sec:chorlesky}

\paragraph{Definition}

Eine Symmetrische $n \times n$-Matrix $A$ heißt \underline{positiv-definit}, wenn für alle $\vec{x}~\in~R^n$, $\vec{x} = 0$ gilt: $\vec{x}^{T} A \vec{x} > 0$.

\paragraph{Satz}

Für jede positiv-definierte Matrix $A$ gibe es genau eine rechts-obere Dreiechsform mit $r_{ii} > 0$ für $i=1,\dots,n$ und $A=R^{T}R$. Diese Zerlegung heißt Cholesky-Zerlegung.

Zum berechnen der Cholesky-Zerlegung geht man wie folgt vor (Pseudo-Code)


\begin{verbatim}
for i = 1,...n,
  s = a_ii - \sum_{k=1}^{i-1} r_{ki}^2 // für i=1 ist s=a_11
  if s \leq 0
    stop // $A$ ist nicht positiv definit
  else
    r_ii = \sqrt{s}
    for j = i+1,...,n // nicht-Diagonalelemente
      r_{ij} = \frac{1}{r_ii} (a_ij - \sum_{k=1}^{i-1} r_{ki}r_{kj})
    endfor
endfor
\end{verbatim}

\paragraph{Bsp}

$A =
\begin{pmatrix}
  4 & 4 & 2\\
  4 & 5 & 5\\
  2 & 5 & 26
\end{pmatrix}
$
$
  R =
  \begin{pmatrix}
    r_{11} & r_{12} & r_{13}\\
    0 & r_{22} & r_{23}\\
    0 & 0 & r_{33}
  \end{pmatrix} =
  \begin{pmatrix}
    2 & 2 & 1\\
    0 & 1 & 3\\
    0 & 0 & 4
  \end{pmatrix}
$
\begin{itemize}
\item $i=1$

    $s = a_{11} = 4 > 0 \implies r_{11} = \sqrt{4} = 2$
    \begin{itemize}
    \item $j=2$

      $r_{12} = \frac{1}{2}(4) = 2$
    \item $j=3$
      $r_{13} = \frac{1}{2}(2 - 0) = 1$
    \end{itemize}
  \item $i=2$

    $s = a_{22} - \sum_{k=1}^{i-1} r_{ki}^2 = 5 - 2^2 =1 > 0 \implies r_{22} = \sqrt{1} = 1$
\begin{itemize}
\item $j=3$

  $r_{23} = \frac{1}{1} (5 - \sum_{k=1}^1 r_{12} r_{12}) = 5 - 2 = 3$
\end{itemize}
\item $i=3$

  $s = a_{33} = a_{33} - \sum_{k=1}^{2} r_{k3}^2 = 26 - (1^2 + 3^2) = 16 > 0 \implies r_{33} = \sqrt{16} = 4$
\end{itemize}

Test:

\begin{align*}
R^TR &=
\begin{pmatrix}
  2 & 0 & 0\\
  2 & 1 & 0\\
  1 & 3 & 4
\end{pmatrix}
\begin{pmatrix}
  2 & 2 & 1\\
  0 & 1 & 3\\
  0 & 0 & 4
\end{pmatrix} =
\begin{pmatrix}
  4 & 4 & 2\\
  4 & 5 & 5\\
  2 & 5 & 26
\end{pmatrix} = A\\
\intertext{aber:}
RR^T &= 
\begin{pmatrix}
  2 & 2 & 1\\
  0 & 1 & 3\\
  0 & 0 & 4
\end{pmatrix}
\begin{pmatrix}
  2 & 0 & 0\\
  2 & 1 & 0\\
  1 & 3 & 4
\end{pmatrix} =
\begin{pmatrix}
  9 & 5 & 4\\
  5 & 10 & 12\\
  4 & 12 & 16
\end{pmatrix} \neq A
\end{align*}

\paragraph{Bemerkung}

Der numerische Aufwand des Cholesky-Verfahrens beträgt $\left(\frac{1}{6} n^3 + \frac{1}{2}n^2 - \frac{2}{3} n\right)$ Punktoperationen und $n$ Wurzelberechnungen.
Das Gauß-Verfahren benötigt $\left(\frac{n^3}{3} + \frac{n}{3}\right)$ Operationen, also für $n \geq 2$ ist Cholesky schenller!

\section{Fehlerrechnung bei LGS}
\label{sec:fehlerlgs}

Wir wollen untersuchen, wie sich Fehler in einem LGS auf dessen Lösung auswirken. Dazu benötigen wir ein Maß.

\paragraph{Definition}

Eine Abbildung  $\| \cdot \|$ $\R^n \to \R$ heißt \underline{Vektornorm}, wenn für alle $\vec{x}, \vec{y} \in R^n$ und alle $\lambda \in \R$ gilt:
\begin{itemize}
\item $\|\vec{x}\| \geq 0$ und $\|\vec{x}\| = 0$ wenn $\vec{x} = 0$.
\item $\|\lambda x\| \geq 0$ und $\|\vec{x}\| = 0$ wenn $\vec{x} = 0$.
\item $\|\vec{x} + \vec{y}\| \leq \|\vec{x}\| + \|\vec{y}\|$
\end{itemize}

Beispiele sind

\begin{itemize}
\item Summenorm (1-Norm) $\|\vec{x}\| = \sum_{i=1}^n |x_i|$
  \item Maximumnorm ($\infty$-Norm) $\|\vec{x}\|_{\infty} = \max\limits_{i=1,\dots,n} |x_i|$
\item euklidische Norm (2-Norm) $\|\vec{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}$
\end{itemize}

Alle Vektornormen sind äquivalent, d.h\@. es existieren $c_1,c_2 \in \R$ so dass

\[
c_1\|\vec{x}\|_a \leq \|\vec{x}\|_b \leq c_2 \|\vec{x}\|_a
\]

D.h\@. falls es eine Vektorfolge in einer Norm $a$ konvergiert, so konvergiert sie auch inder Norm $b$. Man kann entsprechende Matrixnomren definieren für eine $A = (a_{ij})$.
\begin{itemize}
\item Spaltensumme (1-Norm) $\|A\|_1 = \max_{j=1,\dots,n} \sum_{i=1}^n |a_{ij}|$
\item Spektralnorm (2-Norm) $\|A\|_2 = \sqrt{S(A^TA)}$, mit $S(A) = \max\{\lambda\}$ "`Spektralradius von $A$"', $\lambda\colon$ Eigenwerte von $A$.
\item Zeilensummennorm ($\infty$-Norm) $\|A\|_\infty = \max\limits_{i=1,\dots,n} \displaystyle\sum_{j=1}^n |a_{ij}|$
\end{itemize}

\paragraph{Bemerkung}

Matrixnormen erfüllen alle Eigenschaften der zugrundelegenden Vektornormen. Insbesondere auch die Äquivalenz

\[
\|A\vec{x}\|_v \leq \|A\|_v\|\vec{x}|_v
\]

Dann sagt man die Norm ist kompatibel

\paragraph{Satz}

Sei $\|\cdot\|$ eine Norm, $A$ eine reguläre $n\times n$-Matrix die Vektoren $\vec{x}, \vec{x'}, \vec{b}, \vec{b'} \in \R^n$ so dass $A\vec{x} = \vec{b}$ und $A\vec{x'} = \vec{b'}$.

Dann gilt

\begin{align*}
  A\cdot (\vec{x}\vec{x'}) &= \vec{b} - \vec{b'}\\
  \vec{x} - \vec{x'} &= A^{-1}(\vec{b} - \vec{b'})\\
  \|\vec{x}-\vec{x'}\| \leq \|A^{-1}\| \|\vec{b} - \vec{b'}\|
\end{align*}

und entsprechend nach Multiplikation mit
\begin{align}
  \|A\|\|\vec{x}\| \geq \|\vec{b}\| \to   \|\frac{1}{\vec{x}}\| \leq     \frac{\|A\|}{\|\vec{b}\|}\\
  \frac{\|\vec{x} - \vec{x'}\|}{\|\vec{x}\|} \leq \|A\| \|A^{-1}\| \cdot \frac{\|\vec{b}-\vec{b'}\|}{\|\vec{b}\|}
\end{align}

Dann nennt $\|A\| \|A^{-1}\| = \text{cond}(A)$ die \underline{Konditionszahl} der Matrix $A$ bzgl\@. der verwendeten Norm.

\paragraph{Bemerkung}

Die Konditionszahl gibt also die max\@. Verstärkung des realtiven Fehlers (3.2) an, wahrend $\|A^{-2}\|$ die maximale Verstärkung des absoluten Fehlers ist (3.1).

\paragraph{Beispiel}

$A\vec{x} = \vec{b}$ mit $\vec{A} =
\begin{pmatrix}
  3 & -6 \\ -6 & 8
\end{pmatrix}$ und $ \vec{b} =
\begin{pmatrix}
  18\\ 28
\end{pmatrix}
$

\begin{align*}
  \det A &= 24 - 36 = -12\\
  A^{-1} = \frac{1}{\det A} \cdot \text{adj}(A) = \frac{1}{-12}
  \begin{pmatrix}
    8 & -6\\ -6 & 3
  \end{pmatrix} =
  \begin{pmatrix}
    \frac{-2}{3} & \frac{1}{2}\\ \frac{1}{2} & \frac{-1}{4}
  \end{pmatrix}\\
  \|A\|_1 &= \max\{9,14\} = 14 = \|A\|_\infty\\
  \|A^{-1}\| & =\max\left\{\frac{7}{6}, \frac{3}{4}\right\} = \frac{7}{6} = \|A^{-1}\|_\infty\\
\|\vec{b}\|_1 &= 46 & \text{cond} A = \|A\|_1 \|A^{-1}\|
\end{align*}

Wir nehmen an $\|\vec{b}-\vec{b'}\| \leq 0.1$.

Abschätzung absoluter Fehler: $|\vec{x}\vec{x'}| \leq \|A^{-1}\| \|\vec{b} - \vec{b'}\| \leq \frac{7}{6}\cdot 0.1 = 0.11666\dots$.

Relativer Fehler $\frac{\|\vec{x}-\vec{x'}\|}{\|\vec{x}\|} \leq \text{cond} A \frac{\|\vec{b}-\vec{b'}}{\|\vec{b}\|} \leq 16.333\dots  \cdot \frac{0.1}{16} \approx 3.35\%$

Der relativer Fehler hängt ab von cond($A$) und $\frac{1}{\|\vec{b}\|}$!

\paragraph{Satz}

Ist nicht nur die rechte Seite eines LGS fehlerbehaftet, sonder auch die Koeffizientenmatrix, so gilt fúr die Lösung der beiden LGS $A\vec{x} = \vec{b}$ und $A'\vec{x'} = \vec{b'}$ mit $\Delta A = A- A'$ und $\Delta \vec{x} = \vec{x} - \vec{x'}$, $\Delta\vec{b} = \vec{b}-\vec{b'}$

\[
\frac{\|\Delta\vec{x}\|}{\|\Delta\vec{x}\|} \leq \frac{\text{cond}(A)}{1-\text{cond}(A) \frac{\|\Delta A\|}{\|A\|}} \left(\frac{\|\Delta A\|}{\|A\|} + \frac{\|\Delta \vec{b} \|}{\|\vec{b}\|}\right)
\]

\section{Iterative Verfarhen}
\label{sec:iterative-verfahren}

Idee: LGS $Ax = b$ ist äquivalent zu einem vektoriellen Nullstellenproblem $Ax - b = \vec{0}$. Anstelle des Nullstellenrproblems betrachtet man das Fixpunktproblem!

Zerlegung der Matrix $A = I + A - I$ ($I$ ist die Einheitsmatrix). Aus $Ax - b = \vec{0}$ wird


\begin{align*}
  \vec{0} &= (I + A - I)x - b \to Ix = (I - A)x + b\\
  x &= (I-A)x + b
\end{align*}

vektorielle Fixpunktgleichung, kann als Iterationsgleichung interpretiert werden. Startwert $x^{(0)} \to x^{(1)} = (I-A)x^{(0)} + b$ und allgemein

\[
x^{(n+1)} = (I-A)x^{(n)} + b
\]

Fixpunkt der Iterationsgleichung $\hat{=}$ Lösung des LGS.

\paragraph{Beispiel}

$A =
\begin{pmatrix}
  4 & -1 & 1\\
  -2 & 5 & 1\\
  1 & -1 & 5
\end{pmatrix}
$, $b =
\begin{pmatrix}
  5\\ 11\\ 12
\end{pmatrix}
$


\begin{align*}
  x^{(n+1)} &= \left[
    \begin{pmatrix}
      1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1
    \end{pmatrix}
    -
    \begin{pmatrix}
      4 & -1 & 1\\ -2 & 5 & 1\\ 1 & -1 & 5
    \end{pmatrix}
  \right]x^{(n)} +
  \begin{pmatrix}
    5 \\ 11\\ 12
  \end{pmatrix}\\
  x^{(n+1)} &=
  \begin{pmatrix}
    -3 & 1 & -1\\ 2 & -4 & -1\\ -1 & 2 & -4
  \end{pmatrix} x^{(n)} +
  \begin{pmatrix}
    5 \\ 11\\ 12
  \end{pmatrix}
\end{align*}


\begin{center}
  \begin{tabular}{c|c|c|c|c|}
    i & 0 & 1 & 2 & 3\\\hline
    \multirow{3}{*}{$x^{(i)}$} & 0 & 5 & -11 & 22\\\cline{2-5}
    & 0 & 11 & -35 & 148\\\cline{2-5}
    & 0 & 12 & -19 & 29
  \end{tabular}\\
\end{center}
Ansatz nicht konvergent! Betracthe alternive additive Zerlegung $A = A_1 + A_2$

\subsection{Jacobi Verfahren (Gesamtschritt)}
\label{sec:jacobi-gesamt}

Betrachte Zerlegung


\begin{align*}
  A &= L
  \begin{pmatrix}
    0 & 0 & 0 & \cdot\\
    a_{21} & 0 & 0 & \cdot\\
    a_{31} & a_{32} & 0 & \cdot\\
    \cdot & \cdot & \cdot & \cdot
  \end{pmatrix}
  + D
  \begin{pmatrix}
    a_11 & 0 & 0 & \cdot\\
    0 & a_{22} & 0 & \cdot\\
    0 & 0 & a_{33} & \cdot\\
    \cdot &\cdot &\cdot &\cdot
  \end{pmatrix}
  + R
  \begin{pmatrix}
    0 & a_{12} & a_{13} & \cdot\\
    0 & 0 & a_{23} & \cdot\\
    0 & 0 & 0 & \cdot\\
    \cdot &\cdot &\cdot &\cdot
  \end{pmatrix}\\
  Ax-b = 0 &= (L + D + R)x-b \to Dx = -(L+R)x+b\\
  x &= \{\text{FP-Gl}\}-D^{-1}(L+R)x + D^{-1}b \to \{\text{It-Gl}\}x^{(n+1)} = -D(L+R)x^{(n)}  D^{-1} b
\end{align*}

Bsp

\[
A =
\begin{pmatrix}
  4 & -1 & 1\\ -2 & 5 & 1\\ 1 & -2 & 5
\end{pmatrix}
 \to L =
 \begin{pmatrix}
   0  & 0 & 0\\ -2 & 0 & 0\\ 1 & -2 & 0
 \end{pmatrix}
 ,\ D=
 \begin{pmatrix}
   4 & 0 & 0\\ 0 & 5 & 0\\ 0 & 0 & 5
 \end{pmatrix}
 ,\ R=
 \begin{pmatrix}
   0 & -1 & -1\\ 0 & 0 & 1\\ 0 & 0 & 0
 \end{pmatrix}
\]


\begin{align*}
  x^{(n+1)} &= -
  \begin{pmatrix}
    0.25 & 0 & 0\\ 0 & 0.2 & 0\\ 0 & 0 & 0.2
  \end{pmatrix}
  \left[
    \begin{pmatrix}
      0 & -1 & 1\\ -2 & 0 & 1\\ 1 & -2 & 0
    \end{pmatrix} -
    \begin{pmatrix}
      5\\ 11\\ 12
    \end{pmatrix}
  \right]\\
  &=
  \begin{pmatrix}
    0 & 0.25 & -0.25\\ 0.4 & 0 & -0.2\\ -0.2 & 0.4 & 0
  \end{pmatrix} x^{(n)} +
  \begin{pmatrix}
    1.25\\ 2.2\\ 2.4
  \end{pmatrix}
\end{align*}

\begin{tabular}{c|c|c|c|c|c|c|}
  i & 0 & 1 & 2 & 3 & 4 & 5\\\hline
  \multirow{3}{*}{$x^{(i)}$} & 0 & 1.25 & 1.2 & 1.0475 & 1.0065 & 0.9973\\\cline{2-7}
  & 0 & 2.2 & 2.22 & 2.74 & 2.0094 & 1.9986\\\cline{2-7}
  & 0 & 2.4 & 3.03 & 3.048 & 3.0201 & 3.0024
\end{tabular}

\subsection{Einzelschrittverfahren (Gauß-Seidel Verfahren)}
\label{sec:gauss-seidel}

Jakobi-Verfarhen in Komponenten:

%% FIMXE: function numbering per subsection?

\begin{align}
  x_1^{(n+1)} &= 0.25\cdot x_2^{(n)} - 0.25 \cdot x_3^{(n)} + 1.25\\
  x_2^{(n+1)} &= 0.4\cdot x_1^{(n)} - 0.2\cdot x_3^{(n)} + 2.2\\
  x_3^{(n+1)} &= -0.2\cdot x_1^{(n)} + 0.4 \cdot x_2^{(n)} + 2.4
\end{align}

Wenn man annimmt, dass $x^{(n+1)}$ komponentenweise näher am Lösungsvektor liegt, sollte man in Gl (2)(3.4) $x_1^{(n+1)}$ benutzen und in Gl (3)(3.5) $x_1^{(n+1)}$ und $x_2^{(n+1)}$, wie man hierunter sieht.

\begin{align}
  x_1^{(n+1)} &= 0.25\cdot x_2^{(n)} - 0.25 \cdot x_3^{(n)} + 1.25\\
  x_2^{(n+1)} &= 0.4\cdot x_1^{(n+1)} - 0.2\cdot x_3^{(n)} + 2.2\\
  x_3^{(n+1)} &= -0.2\cdot x_1^{(n+1)} + 0.4 \cdot x_2^{(n+1)} + 2.4
\end{align}

In Matrix-Schreibweise

\begin{align*}
  x^{(n+1)} &=
  \begin{pmatrix}
    0 & 0.25 & -0.25\\ 0 & 0 & -0.2\\ 0 & 0 & 0
  \end{pmatrix} x^{(n)} +
  \begin{pmatrix}
    0 & 0 & 0\\ 0.4 & 0 & 0\\ -0.2 & 0.4 & 0
  \end{pmatrix} +
  \begin{pmatrix}
    1.25\\ 2.2\\ 2.4
  \end{pmatrix}
  \intertext{oder:}
  x^{(n+1)} &= D^{-1} \left[ Rx^{(n)} +  Lx^{(n+1)} -b \right]\\
  Dx^{(n+1)} &= -Rx^{(n)} - Lx^{(n+1)} + b & \text{It.-Gl. Gauß-Seidel}\\
  \to (D+L) x^{(n+1)} &= - Rx^{(n)} + b\\
  \to x^{(n+1)} &= -(D+L)^{-1} Rx^{(n)} + (D+L)^{-1}b % box
\end{align*}

\paragraph{In der Praxis}

wird nicht das Inverse der Matrix $(D+L)$ berechnet, sondern das Gl.-Syst\@. $(D+L)x^{(n+1)} = -Rx^{(n)}+ b$ wird gelöst (durch Vorwärtseinsetzen)!

Zurück zum Beispiel

\begin{center}
  \begin{tabular}{c|c|c|c|c|c|}
    & 0 & 1 & 2 & 3 & 4\\\hline
    & 0 & 1.25 & 1.1175 & 1.006 & 1.001\\\hline
    & 0 & 2.7 & 2.001 & 2.007 & 2.0002\\\hline
    & 0 & 3.23 & 2.977 & 3.002 & 2.9998
  \end{tabular}
\end{center}
Konvergiert (etwas) schneller als Gesamtschrittverfahren.

\subsection{Konvergenz von linearen Iterationverfahren}

Gegeben sei eine lineare Matrix-Fixpunkt-Iterationsgleichung $x^{(n+1)} = Bx^{(n)}$ wobei $B$ eine $m \times m$-Matrix, $b \in \R^n$. $\overline{x}$ sei ein Fixpunt mit $\overline{x} = B\overline{x}  b$.

Sei $\|\cdot\|$ ine Matrixnorm (siehe letzte VL), dann gilt: \begin{itemize}
\item[] $\overline{x}$ ist ein anziehenden Fixpunkt, falls $\|B\| < 1$
\item[] $\overline{x}$ ist ein abstoßenden Fixpunkt, falls $\|B\| > 1$
\end{itemize}

Für anziehenden Fixpunkt: Fixpunktiteration konvergiert für \underline{alle} Startwerte.

\paragraph{Bemerkungen}

\begin{itemize}
\item für Gesamtschrittverfahren (Jakobi) gilt $B = -D^{-1}(L+R)$, für die $\infty$-Norm (Zeilensummennorm) gilt (im Hinterkopft behalten, dass $A = D+L+R$):
\begin{align*}
  \|B\|_\infty &= \max\limits_{i=1,m} \sum_{j\neq i} \frac{|a_{ij}|}{|a_ii|} = \max\limits_{i=1,m} \underbrace{\frac{1}{|a_{ii}|}}_{\text{aus $D^{-1}$}} \sum_{j\neq i} \underbrace{|a_{ij}|}_{\text{aus $(L+R)$}} < 1\\
&\iff |a_{ii}| > \sum_{j\neq i} |a_{ij}| \text{ für \underline{alle} $i$}\\
&\hat{=} \text{ "`Zeilensummenkriterium"'}
\end{align*}

So eine Matrix heißt "`diagonaldominant"'

\item Im Einzelschrittverfahren ist $B = -(D+L)^{-1} R$ und man kann zeigen, dass $\| (D+L)^{-1} R \|_\infty \leq \|D^{-1} (L+R)\|_\infty$. \underline{Also}: konvergiert das Gesamtschrittverfahren, so konvergiert auch das Einzelschrittverfahren (Inverse ist nicht immer richtig).
\end{itemize}

\section{Nichtlineare Gleichungsssyteme | Newton Verfahren}
\label{sec:nichtlineare}

Häufig: $n$ nicht-lineare Gleichungen mit $n$ Unbekannten.\\
Gegeben: vektorielle Funktion $f\colon \R^n \to \R^n$.\\
Gesucht: Vektor $\overline{x} \in \R^n$ mit $f(\overline{x}) = \vec{0}$.

\begin{align*}
  f(x) &= f(x_1,\dots,x_n) =
  \begin{pmatrix}
    f_1(x_1,\dots,x_n)\\ f_2(x_1,\dots,x_n)\\ \vdots \\ f_3(x_1,\dots,x_n)
  \end{pmatrix} =
  \begin{pmatrix}
    0\\ 0\\ \vdots\\ 0
  \end{pmatrix}
\end{align*}

Es gibt kein einfaches Verfarhen, um zu prüfen, ob Gl.-Syst\@. lösbar ist! Wie viele Lösungen? $\to$ verallgemenere Newton-Verfahren auf $n$ Dimensionen!

\begin{align*}
  g\colon \R \to \R &\implies g(x) \tilde{=} g(x_0) + (x-x_0)g'(x_0)
  \overset{!}{=} 0\\
  f\colon \R^n \to \R^n &\implies f(x) \tilde{=} f\left(x^{(0)}\right) + D\left[ f\left(x^{(0)}\right)\right] \left(x-x^{(0)}\right)\overset{!}{=} 0
\end{align*}

Lösen des linearen Gl.-Syst\@.

\begin{align*}
  x &= x^{(0)} - \left[ D\left[f\left(x^{(0)}\right)\right]\right]^{-1} f\left(x^{(0)}\right)
\intertext{oder als Iterationsgleichung}
x^{(n+1)} &= x^{(n)} - \left[ D\left[f\left(x^{(n)}\right)\right]\right]^{-1} f\left(x^{(n)}\right)
\end{align*}

Noch mal: nie die inverse einer Matrix berechnen!

%% 20131107

Die Jacobi Matrix der partiellen Ableitungen:

\[
D\left[f(x)\right] = \frac{\partial f_i(x)}{\partial x_j} =
\begin{pmatrix}
  \frac{\partial f_i}{\partial x_1} & \frac{\partial f_i}{\partial x_2} & \dots & \frac{\partial f_i}{\partial x_n}\\
  \vdots \\
  \frac{\partial f_n}{\partial x_1} & & \dots & \frac{\partial f_n}{\partial x_n}
\end{pmatrix}
\]

\paragraph{Bsp}

\begin{align*}
  f(x_1,x_2) =
  \begin{pmatrix}
    2x_1 + 4x_2\\ 4x_1 + 8x^3_2
  \end{pmatrix} \to D[f] =
  \begin{pmatrix}
    2 & 4\\ 4 & 24x^2_2
  \end{pmatrix}
\end{align*}

wähle als Startwert
\begin{align*}
  x^{(0)} &=
  \begin{pmatrix}
    4\\ 2
  \end{pmatrix} \to D[f(x_0)] =
  \begin{pmatrix}
    2 & 4\\ 4 & 96
  \end{pmatrix}\\
    f(x^{(0)}) &=
    \begin{pmatrix}
      16\\ 80
    \end{pmatrix}\\
    D[f(x^{(0)}](x-x^{(0)}) &\overset{!}{=} -f(x^{(0)})\\
    \begin{pmatrix}
      2 & 4\\ 4 & 96
    \end{pmatrix} (x-x^{(0)}) &=
    \begin{pmatrix}
      -16\\ -80
    \end{pmatrix}
\end{align*}

\begin{tabular}{c|c|c|c|c}
  & $n=1$ & 2 & 3 & 4\\\hline
  \multirow{2}{*}{$x^{(n)}$} & -2.91 & -2.30 & -2.-5 & -2.008\\
  & 1.45 & 1.15 & 1.03 & 1.009
\end{tabular}

Iteration konvergiert schnell gegen $\overline{x} =
\begin{pmatrix}
  -2 \\ 1
\end{pmatrix}
$, eine von 3 Lösungen.

\paragraph{Satz}

Das Newton-Vefahren konvergiert quadratisch, wenn:
\begin{itemize}
\item $x^{(0)}$ nahe genug an $\overline{x}$ liegt
\item $D\left[f(\overline{x})\right]$ regulär ist
\item $f$ dreimal stetig differenzierbar ist
\end{itemize}

\paragraph{Achtung}

Nicht immer konvergiert das Newton-Verfahren gegen einie Nullstelle. Bsp:

\[
f =
\begin{pmatrix}
  x_1^3-x_2-1\\ x_1^2-x_2
\end{pmatrix}
\]

Startwert $x^{(0)} =
\begin{pmatrix}
  -1\\ 0
\end{pmatrix}
$ konvergiert gegen $\overline{x} =
\begin{pmatrix}
  0\\ -0.5
\end{pmatrix}
$ aber $f(\overline{x}) =
\begin{pmatrix}
  -0.5\\ 0.5
\end{pmatrix}
$! Grund:

\begin{align*}
  D[f] &=
  \begin{pmatrix}
    3x_1^2-1\\ 2x_1-1
  \end{pmatrix}
  \intertext{und}
  D[f(\overline{x})] &=
  \begin{pmatrix}
    0 & -1\\ 0 & -1
  \end{pmatrix}
\end{align*}

$\implies$ Lösung der Iterations-Gleichung checken auf $f(\overline{x}) = 0$.

\paragraph{Bemerkung}

in der Praxis werden partielle Ableitungen durch differenzen angenährt:

\[
D[f(x)]_{ij} = \frac{f_i(x + he_j) - f_i(x)}{h}
\]

mit $e_j$ Einheitsvektor in $j$-Richtung.

\subsection{Vereinfachtes Newton-Verfahen}
\label{sec:vereinnewt}

Rechenaufwan pro Schritt minimiert, wenn $D[f(x^{(0)}]$ benutzt wird. Konvergenz nur noch linear!

\chapter{Interpolation \& Ausgleichsrechnung ("`Fitten"')}
\label{chap:fitten}

In vielen Anwendungen: Messdaten sollen durch eine Formel beschrieen werden, zum Beispiel um Integrale, Differentiale etc\@. zu berechnen.

\paragraph{Def}

gegeben $n+1$ Wertepaare $(x_i,f_i)$ mit $i=0,\dots,n$; gesucht stetige Funktion $f(x)$ mit $f(x_i) = f_i$ für \underline{alle} $i=0,\dots,n$.

\begin{itemize}
\item $\{x_n\}$ Stützstellen
\item $\{f_n\}$ Stützwerte
\item $\{x_n,f_n\}$ Stützpunkte
\item $f(x)$ ist die Interpolierende der Stützpunkte
\end{itemize}
\paragraph{Bemerkung}

Interpolationsproblem ist nicht eindeutig!


\section{Polynom-Interpolation}
\label{sec:interpo}

Polynom $n$-Ren Grades $\to n+1$ Freiheitsgrade, es ist also möglich die $n+1$ Koeffizienten so zu wählen, dass $f(x_i) = f_i$ ist!

\paragraph{Satz}

gegeben $n+1$ Wertepaare $(x_i,f_i)$. Es gibt genau ein Polynom vom Grad höchstens $n$, so dass $p(x_i) = f_i$ für alle $i$.

$n = 1$: lineare Interpolation, $n=2$: quadratische Interpolation, usw.

\paragraph{Ineffiziente Methode}

explizite Lösung eines linearen Gl.-Syst\@. $p(x)= \sum_{i=0}^3 a_i x^i$, $f_i = \sum_{i=0}^3 a_i x_j^i$

\begin{tabular}{c||c|c|c|c}
  $x_i$ & -1 & 0 & 1 & 2\\\hline
  $f_i$ & 5 & -2 & 9 & -4
\end{tabular}

4 Gleichungen für 4 Unbekannte:

\begin{align*}
  5 &= a_0 + a_i(-1) + a_2(-1)^2 + a_3(-1)^3\\
  -2 &= a_0 + \cancel{a_i\cdot(0)}, \dots\\
  9 &= a_0 + a_i(1) + a_2(1)^2 +a_3(1)^3\\
  -4 &= a_0 + a_i(2) = a_2(2)^2 + a_4(2)^3
\end{align*}

Als matrizen

\begin{align*}
  \begin{pmatrix}
    5\\-2\\9\\=4
  \end{pmatrix} &=
  \begin{pmatrix}
    1 & -1 & 1 & -1\\
    1 & 0 & 0 & 0\\
    1 & 1 & 1 & 1\\
    1 & 2 & 4 & 8
  \end{pmatrix}
  \begin{pmatrix}
    a_0\\ a_1\\a_2\\a_3
  \end{pmatrix}
\end{align*}

Lösung (siehe VL): $a_0 = -2$, $a_1=9$, $a_2=9$, $a_3=-7$, $p(x) = -2+9x+9x^2-7x^3$

[Graph davon]

\paragraph{Lagrange Form des Interpolationspolynoms}

\begin{align*}
  p(x) &= \sum_{i=0}^n f_i l_i(x)
  \intertext{mit}
  l_i(x) &= \prod_{\substack{j=0\\j\neq i}}^n \frac{x - x_j}{x_i-x_j}
\end{align*}

in unserem Beispiel

\begin{align*}
  l_o(x) &= \left(\frac{x-x_1}{x_0 - x_1}\right)\left(\frac{x-x_2}{x_0 - x_2}\right)\left(\frac{x-x_3}{x_0 - x_3}\right)
\end{align*}

$l_0(x)$ ist Null an allen Stützstellen außer an $x_0$, und $l_0(x_0) = 1$. (analog für alle anderen Polynome $l_1,l_2,l_3$). $\to p(x_i) = f_i$!!

\begin{align*}
  l_o(x) &= \left(\frac{x-0}{-1 - 0}\right)\left(\frac{x-1}{-1 - 1}\right)\left(\frac{x-2}{-1 -2}\right) = \frac{-x(x-1)(x-2)}{6}
\end{align*}

jedes $l_i(\star)$ ist ein Polynom dritten Grades $\to p(x_0)$ auch dritten Grad. Berechnung von $p(x)$ durch Summation von $l_i(x)$ immer noch zu aufwändig $\hat{=}$ es gibt schnellere Methode!

\paragraph{Newtonsche Interpolationsformel (dividierte Differenzen)}

Gegeben Wertpaare $(x_i,f_i)$ für $i=0,\dots,n$. Berechnung von "`dividierende Differenzen"':

\begin{itemize}
\item[] für $k=1\dots n$
  \begin{itemize}
  \item[] für $i=0\dots n-k$
    \begin{itemize}
    \item[] $f(x_i,x_{i+1}, \dots, x_{i+k}) = \frac{f(x_i+1, \dots, x_{i+k}) - f(x_i \dots x_{i+k-1})}{x_{i+k} - x_i}$
    \end{itemize}
  \item[] ende für
  \end{itemize}
\item[] ende für
\end{itemize}

im Schema für $n=2$.

\begin{tabular}{c|c|cccccc}
  $x_i$ & $f_i$  & $f(x_i)$\\\cline{1-3}
  $x_0$ & $f(x_0)$\\
  & & $f(x_0,x_1) = \frac{f(x_1) - f(x_0)}{x_1 - x_0}$\\
  $x_1$ & $f(x_1)$ & & $f(x_0, x_1, x_2) = \frac{f(x_2,x_1) - f(x_0,x_1)}{x_2-x_0}$\\
  & & $f(x_1,x_2) = \frac{f(x_2) - f(x_1)}{x_2 - x_1}$\\
  $x_2$ & $f(x_2)$
\end{tabular}

\paragraph{Bem.}

\begin{itemize}
\item $x_i$ müssen nicht nach Größe sortiert sein
\item neue Wertepaare angefügt werden
\end{itemize}

für unser Beispiel

\begin{tabular}{c|c|c|cccccc}
  $i$ & $x_i$ & $f(x_i)$ & $f(x_i,x_{i+1})$ & $f(x_i,x_{i+1},x_{x+2})$ & $f(x_i,x_{i+1},x_{x+2},x_{i+3})$\\\hline
  0 &  -1 & 5\\
  & & & $f(x_0,x_1) = \frac{-2-5}{0-(-1)} = -7$\\
  1 & 0 & -2 & & $f(x_0,x_1,x_2) = \frac{11-(-7)}{1-(-1)} = 9$\\
  & & & $f(x_1,x_2) = \frac{9-(-2)}{1-0} = 11$ & & $\frac{-12-9}{2-(-1)} = -7$\\
  2 & 1 & 9 & & $f(x_1,x_2,x_3) = \frac{-13-11}{2-0} = -12$\\
  & & & $f(x_2,x_3) = \frac{-4 -9}{2-1} = -13$\\
  3 & 2 & -4
\end{tabular}

\paragraph{Interpolationsformel}

\begin{align*}
  p(x) =& \sum_{i=0}^n f(x_0,\dots,x_i)\prod_{j=0}^{i-1} (x-x_j)\\
  =& \underbrace{f(x_0)}_{i=0} + \underbrace{f(x_0,x_1)(x-x_0)}_{i=1}\\
&+ \underbrace{f(x_0,x_1,x_2)(x-x_0)(x-x_1)}_{i=2} + \underbrace{f(x_0,x_1,x_2,x_3)(x-x_0)(x-x_1)(x-x_3)}_{i=3}
\end{align*}

Nur die divedierten Differenzen im oberen schrägen Zeile tragen bei!

Für unser Beispiel:

\begin{align*}
  &= 5 + (-7)(x-(-1)) + 9(x-(-1))(x-(x-0)+(-7)(x+1)x(x-1)\\
  &= 5 -7x - 7 + 9x^2 + 9x -7(x^3 - x)\\
  &= \boxed{-7x^3 + 9x^2 + 9x - 2}
\end{align*}

Umformung von $p(x)$ durch Ausklammern:

\begin{align*}
  p(x) &= f(x_0) + (x-x_0)\left[ f(x_0,x_1) + (x-x_1)\left[ f(x_0,x_1,x_2) + (x-x_2)f(x_0,x_1,x_2,x_3) \right] \right]
\end{align*}

\paragraph{Iterative Auswertung des Newton Polynoms (Horner Schema)}

\begin{itemize}
\item[] $r_n = f(x_0,x_1,\dots,x_n)$
\item[] für $k = n-1$ bis $0$
  \begin{itemize}
  \item[] $r_k = r_{k+1} (x-x_k)+f(x_0,\dots,x_k)$
  \end{itemize}
\item[] ende für
\item[] $p(x) = r_0$
\end{itemize}

Man benötigt zur Auswertung des Newton Polynoms nur $n$ Punktoperationen!

\paragraph{Bsp}
($n=3$)

\begin{itemize}
\item $r_3 = f(x_0,x_1,x_2,x_3) = -7$
\item $r_2 = r_3(x-x_2) + f(x_0,x_1,x_2) = -7(x-1)+9$
\item $r_1 = r_2(x-x_1) + f(x_0,x_1) = [-7(x-1)+0] x -7$
\item $r_0 = r_1(x-x_0) + f(x_0) = [(-7(x-1) + 9)x-7](x+1)+5$
\end{itemize}

was das Polynom anders geschrieben ist.

\paragraph{Problem bei Polynom-Interpolation}

gegeben eine Funktion $f(x) = \frac{1}{1+x^2}$.

\begin{tabular}{c|c|c|c|c|c|c|c|c}
  $x_i$ & -3  & -2  & -1  &  0 & 1 & 2 & 3 &\\\hline
  $f_i$ & 0.1 & 0.2 & 0.5 & 1  &  &  &  &
\end{tabular}
% \begin{gnuplot}[terminal=cairolatex]
%   plot 1/(1+x^2)
% \end{gnuplot}
[Graph]

gerade Funktion: $p(x) = 1-0.64x^2 + 0.15x^4 - 0.06x^6$

Polyno weist an den Rändern des Stützstellenintervalls Überschwinger auf. Erhöhung von $n$ verbessert die Situtation nicht. Außerdem divergiert Polynom außerhalb des Intervalls $\implies$ Interpolationspolynome nicht für Extrapolation geeignet.

\section{Spline Interpolation}
\label{sec:spline}

Ausgangsüberlegung: gute Näherung einer Funktion $\implies$ viele Stützstellen. Aber: Polynom hoher Ordnung $\implies$ viele Schwingungen + starke Divergenz.

\paragraph{Idee}

teile Intervall in Teilintervale auf, benutze jeweils Polynome niedriger Ordnung!

\paragraph{Einfachste Stufe}

Gerade, die durch die Datenpunkte durchgeht. Eine lineare Spline Interpolation. "`Kniche"' häufig störend in Anwendungen $\to$ fordere stetige erste und zweite Ableitungen $\to$ "`kubische Splines"'

\paragraph{Def}

eine Funktion $s(x)\colon [a,b] \to \R$ heißt kubischer Spline zu den Stützstellen $a=x_0~<~x_1\dots < x_n=b$ falls gilt:
\begin{itemize}
\item $s''(x)$ existiert und ist stetig
\item $s(x)$ ist auf den Intervallen $[x_i,x_{i+1}]$ jeweils ein Polynom dritten Grades.
\item interpolierender Spline zu den Werten $f_0\dots f_n$: $s(x_i) = f_i$ für $i=0,\dots,n$
\item periodischer Spline: zusätzlich $s(a) = s(b)$ und $s'(a) = s'(b)$ und $s''(a) = s''(b)$ oder
\item natürlicher Spline: zusätzlich $s''(a) = s''(b) = 0$
\end{itemize}

\paragraph{Bestimmung der Koeffizienten}

ein Polynom dritten Grades hat vier Koeffizienten $\hat{=}$ Freiheitsgrade; für $n$ Intervalle haben wir also $4n$ Koeffizienten!

\begin{itemize}
\item Pro Intervall 2 Interpolationsbedingungen (rechts und links) $\to 2n$.
\item An den Stützstellen $x_1,\dots,x_{n-1}$ sollen 1\@. und 2\@. Ableitungen gleich sein.
\end{itemize}

Deswegen 2 zusätzliche Bedingungen, zusammen \underline{$4n-2$ Bed.}

%% 20131119

\paragraph{lineare Ausgleichrechnungen}

$f = \sum_{i=1}^{m} a_i f_i (x)$ für $(x_j, y_j)$ für $j = 1\dots n$
(wobei, $n > m$).

Fehlerfunktional

\begin{align*}
  A_{ij} &= f_i(x_i)\\
  E &= \sum_{i=1}^{n} (y_i - f(x_i))^2\\
  &= \sum_{i=1}^{n} (y_i) = \sum_{j=1}^{m} a_j f_j (x_i)^2 = (\overset{\rightharpoonup}{y} - \overset{\leftrightarrow}{A} \vec{a})
\end{align*}

Fehlergleich. System
\[
\overset{\leftrightarrow}{A} \vec{a} =  y \iff E = 0
\]

Normalgleicungen 

\[
\frac{\partial (\{a_m\})}{\partial a_i} = 0
\]

mit $i=1,\dots,m$

$\to$ Normalgleichungssytem

\[
\boxed{A^T A a = A^T y}
\]

\paragraph{Herleitung}

\begin{align*}
  E(\{a_m\}) &= (\overset{\rightharpoonup}{y} - \overset{\leftrightarrow}{A} \vec{a})^2 = \sum_{i,j,k} (y_i - A_{ij} a_j)(y_i-A_{ik} a_k)\\
&= y_i^2 - A_{ij} a_j y_i - y_i A_{ik} a_k + A_{ij} a_j A_{ik} a_k\\
&= y_i^2 - 2A_{ij} a_j y_i + A_{ij} a_j - A_{ik} a_k\\
\frac{\partial E}{\partial a_l} &= -2A_{il} y_i + A_{il} A_{ik} a_k + A_{ij} a_j A_{il}\\
&= -2A_{il} y_i + 2A_{il} A_{ik} a_k = 0 \to A_{il} A_{ik} a_k = A_{il}y_i\\
\leftrightarrow& A^T_{li} A_{ik} a_k = A^T_{li} y_i \leftrightarrow \boxed{\underbrace{\overset{\leftrightarrow}{A}^T\overset{\leftrightarrow}{A}}_{m\times m} \underbrace{\rharp{a}}_{m} = \underbrace{\overset{\leftrightarrow}{A}^T}_{\text{$m$ Komp}} \underbrace{\rharp{y}}_{\text{$n$ Komp}}}
\end{align*}

\paragraph{Bem}

Fehlergl.Syst. hat ?$n$ gleich. (= Wertpaare), aber nur $m$ Unbekannte (= Anpassungparameter). 

$n > m \to$ Fehl.Gl.Syst. überbestimmt $\hat{=}$ i.A\@. keine Lösung!
\begin{itemize}
\item Man bezeichnet $r = Aa-y$ als Residuumsvektor, i.A\@. $\vec{r} \neq 0$! Im Fall $\vec{r} = 0$ ist $\overset{\rightharpoonup}{a}$ Lösung des Gehlergleichungssystems und es liegt eine perfekte Modellanapassung vor ($\hat{=}$ also Interpolation). Typischerweise der Fall wen $m = n$.

(außer wenn Anpassfunktion oder Wertepaare redundant sind\dots)
\item Lösung des Ausgleichungsproblems: Fehlerfunktional minimal bzgl\@. $\rharp{a}$

$\hat{=}$ Lösung des Normalgleich.Systems

$\hat{=}$ lineares Gl.Syst. mit sym. Matrixe $A^TA$ und Vektor $A^Ty\to$ Cholesky Dokomposition einsetzbar.

\fbox{Lösungsvektor $\rharp{a}$}

\begin{align*}
  B_{ik} = A^T_{ij} A_{jk} \overset{!}{=} A^T_{kj} A_{ji} = A_{jk} A^T_{ij} = A^T_{ij} A_{jk}
\end{align*}
\end{itemize}

\paragraph{Zurück zum Beispiel}

\[
f_1(x) = x, \quad f_2(x) = 1
\]

\begin{tabular}{c|cccc}
  $x^i$ & 1 & 2 & 3 & 4\\\hline
  $y^i$ & 6 & 6.8 & 10 & 10.5
\end{tabular}

\begin{align*}
  A &=
  \begin{pmatrix}
    f_1(x_1) & f_2(x_1)\\
    f_1(x_2) & f_2(x_2)\\
    f_1(x_3) & f_2(x_3)\\
    f_1(x_4) & f_2(x_4)
  \end{pmatrix} =
  \begin{pmatrix}
    11 & 1\\
    2 & 1\\
    3 & 1\\
    4 & 1
  \end{pmatrix}\\
  A^TA &=
  \begin{pmatrix}
    1 & 2 & 3 & 4\\
    1 & 1 & 1 & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 1\\ 2 & 1\\ 3 & 1\\ 4 & 1
  \end{pmatrix} =
  \begin{pmatrix}
    30 & 10\\
    10 & 4
  \end{pmatrix}\\
  A^T y &=
  \begin{pmatrix}
    1 & 2 & 3 & 4\\
    1 & 1 & 1 & 1
  \end{pmatrix} =
  \begin{pmatrix}
    6\\ 6.8\\ 10\\ 10.5
  \end{pmatrix} =
  \begin{pmatrix}
    91.6\\ 33.3
  \end{pmatrix}
  \intertext{und}
  A^TAa = A^Ty &\hat{=} \begin{pmatrix}
    30 & 10\\
    10 & 4
  \end{pmatrix}
  \begin{pmatrix}
    a_1\\ a_2
  \end{pmatrix} =
  \begin{pmatrix}
    91.6\\ 33.3
  \end{pmatrix}
\end{align*}

Das gleiche Gl.System wie letzte VL!

Nicht-linearer Funktionasansatz $f_1(x) = x^2$, $f_2(x) = 1$ aber \underline{lineare} Ausgleichsrechnung!

\begin{align*}
  A &=
  \begin{pmatrix}
    1 & 1\\ 4 & 1\\ 9 & 1\\ 16 & 1
  \end{pmatrix}, \quad
    A^TA =
    \begin{pmatrix}
      1 & 4 & 9 & 16\\
      1 & 1 & 1 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 & 1\\ 4 & 1\\ 9 & 1\\ 16 & 1
    \end{pmatrix} =
    \begin{pmatrix}
      354 & 30\\
      30 & 4
    \end{pmatrix}\\
A^Ty &=   \begin{pmatrix}
    1 & 1\\ 4 & 1\\ 9 & 1\\ 16 & 1
  \end{pmatrix}
  \begin{pmatrix}
    6\\ 6.8\\ 10\\ 10.5
  \end{pmatrix} =
  \begin{pmatrix}
    291.2\\ 33.3
  \end{pmatrix}
\end{align*}

Ausgleichsproblem immer noch linear

\[
A^TA\rharp{a} = A^T \rharp{y} \to
\begin{array}{c}
  a_1 = 1.296\\
  a_2 = -5.59
\end{array}
\quad f(x) = a_1x^2 + a_2
\]

für unser Bieispiel: linearer Fit "`besser"' $\hat{=}$ $|r|$ kleiner!

%% should be 4.4
\section{Nicht-lineare Ausgleichsprobleme, Gauß-Newton Verfahren}

\paragraph{Bspl (unecht)}

betrachte alte Wertepaare
\begin{tabular}{c|cccc}
  $x_i$ & 1 & 2 & 3 & 4\\\hline
  $y_i$ & 6 & 6.8 & 10 & 10.5
\end{tabular} und die Funktion $f(x) = ae^{bx}$ mit Anpassungsparametern $a$ und $b$. $f(x)$ hängt nichtlinear von $b$ ab!

\paragraph{Trick}

betrachte
\begin{align*}
  \ln f(x) &= \ln a  \ln e^{bx}\\
  &= \ln a + bx
\end{align*}

und logarithmische Funnktionswerte

\begin{tabular}{c|cccc}
  $x_i$ & 1 & 2 & 3 & 4\\
  $y_i$ & 1.72 & 1.52 & 2.30 & 2.35
\end{tabular}

$\to$ lineares Problem in logarithmischen Skalierung

\begin{align*}
  f(x) = ax^b \to \boxed{\ln f = c + bx}
\end{align*}

ähnlich:

\begin{align*}
f(x) = ax^b \to \ln f &= \ln a + b \ln\\
&= c + b \tilde{x}
\end{align*}

\paragraph{Def allgemeines Ausgleichungsproblem}

gegeben Ansatzfkt. $f_a(a_1,\dots a_m, x)$ und Wertepaare $(x_i, y_i)$ mit $i = 1\dots n$ Fehlerfunktional $E(a_1,\dots a_m) = \sum_{i=1}^n (y_i - f_a(a_1,\dots a_m, x_i))^2$. Ausgleichsproblem: Minimiere $E$ im erlaubten Parameterraum.

Bermerkung
\begin{itemize}
\item lineares Problem im allgem. Fall enthalten.
\item im Prinzip kann man das nicht-lineare Gl.Systl. $\frac{\partial E}{\partial a_i} = 0$ mit dem Newton-Verfahren lösen; Nachteil: instabil und 2. partielle Ableitung werden benötigt!
\end{itemize}

\paragraph{$\to$ Gauß-Newton Verfahren}

definiere Vektor $\vec{g}(a_1,\dots a_m) =
\begin{pmatrix}
  y_1 - f_a(a_1, \dots a_m,x_1)\\
  \vdots\\
  y_n - f_a(a_1, \dots a_m, x_n)
\end{pmatrix}$ mit $g\colon \R^m \to \R^n$

Fehlerfunktional $E(\rharp{a}) = g^2$, $E\colon \R^m \to \R$, $g_i = y_i - f_a(a_1,\dots a_m, x_i)$

Problem: finde \rharp{a} so dass $E(\vec{a}) = \vec{g}(a)^2$ minimal wird $\hat{=}$ Quadratmittelproblem.

(also: nicht-lin\@. Ausgl\@. rechn\@. $\hat{=}$ Quadratmittelproblem)

\paragraph{Idee}

\begin{itemize}
\item Ersetze $g(\vec{a})$ in $E$ durch Linearisierung.
\item minimiere den linearen Ausdruck ($\hat{=}$ lineare Ausgleichungsrech.)
\item iteriere diesen Prozess, starte mit Schätzwert $\vec{a}^{(0)}\dots$ (wie beim Newton-Verfarhen\dots)
\end{itemize}

Starte mit Schätzwert $\vec{a}^{(0)}$. Linearisiertes Ausgleichsproblem definiert durch

\[
E(\vec{a}) = \left(g\left(\vec{a}^{(0)}\right) + D\left[ g\left(\vec{a}^{(0)}\right) \right] \left(\vec{a} - \vec{a}^{(0)}\right)\right)^2
\]

Berechne $\delta^{(n)} = a^{(n-1)} - a^{(n)}$ als Lösung des lin. Ausgl.Probml.

\[
\min\limits_{\delta^{(0)}} \left(g\left(\vec{a}^{(0)}\right) + D\left[ g\left(\vec{a}^{(0)}\right) \right] \delta^{(0)} \right)^2
\]

Setze $\un{\vec{a}}{n+1} = \un{\vec{a}}{n} + \un{\vec{\delta}}{n}$ .Zu lösen ist das Normalgleich.-Syst.:
\[
D^T \left[ g\right(a^{(n)}\left) \right] D\left[g\left(a^{(n)}\right)\right] \delta^{(n)} = - D^T\left[g\left(a^{(n)}\right)\right] g\left(a^{(n)}\right)
\]

%% 20131121

in der Praxis benutzt man ein gedämpftes Gauß-Newton Verfahren.
\begin{itemize}
\item Wieder berechne $\un{\delta}{n}$ als Lösung von $\min\limits_{\un{\vec{a}}{n}} \left( g\left(\un{a}{n}\right) + D\left[g\right] \un{\delta}{n}\right)^2$
\item Bestimmung des Dämpfungsfaktor: wähle die größte Zahlt $t \in \{ 1, \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \dots \}$ für die mit $\phi(t) = \left( g\left( \un{a}{n} + t\un{\delta}{n}\right)\right)^2$ gilt $\phi(t) < \phi(0)$
\item Setze $\un{a}{n+1} = \un{a}{n} + t \un{\delta}{n}$ und iteriere!
\end{itemize}

\paragraph{Bemerkung}

Der Dämpfungsfaktor $t$ erhöht den Einzugsbereich des Verfarhens, d.h\@. das gedämpfte Verfahren konvertiert für einen größeren Bereich von Startvektoren \un{\vec{a}}{0} (vergliechen mit $t=1$, ungedämpft).

\begin{itemize}
\item Fehler viel geringen in jedem Schritt
\[
\left(\vec{g}\left(\un{\vec{a}}{n+1}\right)\right)^2 = \phi(t) < \phi(0) = \left(\vec{g}\left(\un{\vec{a}}{n}\right)\right)^2
\]
(aber: Konvergenz zum \underline{echten} Minimum nicht garantiert)
\item Abbuchbedingung $\sqrt{\left(t\un{\vec{\delta}}{n}\right)^2} < \text{TOL}$ (aber: bedeutet nicht, dass ich mich im Abstand TOL vom gesuchtem Minimum befinde!)
\end{itemize}

\paragraph{Beispiel}

Ansatzfunktion $f(x) = a_1 l^{a_2x}$. Die Vektorfunktion $g(a_1,a_2)$ lautet $g_i(a_1,a_2) = y_i - a_1 l^{a_2x_i}$

\begin{tabular}{c|ccccc}
  $x_i$ & 0 & 1 & 2 & 3 & 4\\\hline
  $y_i$ & 3 & 1 & 0.5 & 0.2 & 0.05
\end{tabular}

Jakobi Matrix:

\[
D[g] = \frac{\partial g_i(a_m)}{\partial a_j} = \left( -l^{a_2x_i}, -a_1x_il^{a_2x_i} \right)
\]

Wir erwarten $a_1 > 0$, $a_2 < 0$, ungedämpft ($t=1$).

\begin{tabular}{c|ccccc}
  $i$ & 0 & 1 & 2 & 3 & 4\\\hline
  \multirow{2}{*}{$\un{\vec{a}}{i}$} & 1 & 2.99 & 1.26 & 2.91 & 2.98\\
  & -1.5 & 0.32 & 0.279 & -0.86 & -1.00
\end{tabular}

gedämptes Verfahren

\begin{tabular}{c|cccccc}
  $i$ & 0 & 1 & 2 & 3\\\hline
  \multirow{2}{*}{$\un{\vec{a}}{i}$} & 1 & 1.99 & 2.92 & 2.98 \\
  & -1.5 & -0.56 & -0.951 & -0.999
\end{tabular}

\begin{tabular}{c|cccccccc}
  $i$ & 0 & 1 & 2 & 3 & \dots & 10\\\hline
  \multirow{2}{*}{$\un{\vec{a}}{i}$} & 2 & 0.003 & 0.004 & 0.207 & \dots & 1.98\\
  & 2 & 2.00 & 1.75 & -0.79 & \dots & -1.00
\end{tabular}

degämpfter Verfahren ist schneller \& konvergenter.

\chapter{Numerische Differentiation / Integration}
\label{chap:numdiff}

\section{Numerische Differentiation}

In vielen Anwendungen brauch man höhere Ableitungen $ f'(x_0), f''(x_0)$ einer Funktion $f(x)$ benötigt, aber diese Funktonen stehen nicht zur Verfügung $\to$ Näherungen!

\paragraph{Def}

Ableitung als Grenzwert eines Differenzenquoteienten

\[
f'(x_0) = \lim\limits_{x\to x_0} \frac{f(x) - f(x_0)}{x-x_0} \approx \frac{f(x+h) - f(x)}{h} \equiv D_1f(x_0,h)
\]

 wobei $h = x - x_0$. $D_1f$: Differenzenformel oder finite Differenz 1\@. Ordnung. Herleitung genaurer Differenzformel \& Fehlerabschätzung mit Taylor:

\begin{align*}
  f(x_0 + h) &= f(x_0) + hf'(x_0) + \frac{1}{2} h^2 f''(x_0) +     \frac{1}{6} h^3 f'''(x_0)+ \dots\\
f(x_0 - h) &= f(x_0) - hf'(x_0) + \frac{1}{2} h^2 f''(x_0) -     \frac{1}{6} h^3 f'''(x_0)+ \dots
\end{align*}

Fehlerabschätzung:
\begin{align*}
  hf'(x_0) &= f(x_0 + h) - f(x_0) - \frac{1}{2}h^2 f''(x_0) + \dots\\
  \to f'(x_0) &= \frac{f(x_0 + h) - f(x_0)}{h} - \frac{1}{2} h f''(x_0)+\dots\\
f'(x_0) &= D_1 [f(x_0, h)] - \frac{1}{2}hf''(x_0) + \dots
\end{align*}

\paragraph{Def Diskretisierunsfehler}

Bei Differnzenformel für $f'(x_0)$ bezeichnet man den Fehler $| D_f(x_0, h) - f'(x_0) |$ als Diskretisierungs- oder Abschneidefehler. Die Formel $Df$ hat die Fehlerordnung $k$ falls es ein $C > 0$ gibt, so dass für kleines $h$ gilt: $|D_f(x_0, h) - f'(x_0)|\leq Ch^k$, kurzschreibweise $\bO(h^k)$

In unserem Beispiel $D_1 \to |D_1[f(x_0,h)] - f'(x_0)| \approx \frac{1}{2} h f''(x_0) \leq Ch^k$ mit $k=1$ also Fehlerordnung $k=1$! Formeln mit höherer Ordnung besser, da bei Halbierung von $h$ der Fehler um Faktor $(\frac{1}{2})^k$ kleiner wird (Vorfaktoren $C$ sind typischerweise nicht so wichtig\dots)

\paragraph{Herleitung besserer Formeln für Differentiale}

\begin{align*}
  f(x_0 + h) - f(x_0 - h) &= 2 h f'(x_0) + \frac{1}{3} h^3 f'''(x_0) + \dots\\
\to D_2 f(x_0,h) &:= \frac{f(x_0+h) - f(x_0-h)}{2h}\\
\text{und } D_2f(x_0,h) - f'(x_0) &= \frac{1}{6}h^2f''(x_0)
\end{align*}

$D_2f$ ist also eine Differnzenformel der Fehlerordnung $k=2$ für $f'(x)$ (außerdem  eine symmetrische oder Zentrale Differnzformel).

Höhere Ableitungen: addiere beide Tayler-Ausdrücke
\begin{align*}
  f(x_0+h) + f(x_0 -h ) &= 2f(x_0) + h^2 f''(x_0) + \frac{1}{12} h^4     f^{iv}(x_0) + \dots\\
\to f''(x_0) &= \frac{f(x_0 + h) - 2f(x_0)+f(x_0-h)}{h^2} - \frac{1}{12} h^2 f^{iv}(x_0)
\end{align*}

\paragraph{Balance von Diskretisierungs- \& Rundungsfehlern}

Bsp:

Formel $D_1f = \frac{f(x_0 + h) - f(x_0)}{h}$ für $f(x) = \sin(x)$ und $x_0 = 1$.

\begin{align*}
  f'(x) = \cos(x), f'(1) = 0.54\dots
\end{align*}

Für 10-stellige dezimale Rechnung

\begin{tabular}{c|l}
  $h$ & $|D_1f(x_0=1,h) - \cos(x_0=1)|$\\\hline
  $10^{-1}$ & 0.043\\
  $10^{-2}$ & 0.004\\
  $10^{-3}$ & 0.0004\\
  $10^{-4}$ & 0.00004\\
  $10^{-5}$ & 0.000002\\
  $10^{-6}$ & 0.000002\\
  $10^{-7}$ & 0.0003\\
  $10^{-9}$ & 0.04\\
  $10^{-10}$ & 0.54\\
  $10^{-12}$ & 0.54
\end{tabular}

für abnehmenden $h$ wird der Fehler zunächst kleiner, steigt dann wieder an, und bleibt für $h < 10^{-10} = \varepsilon$. Kommt auf, da $x_0 = 1 = 1 + h$ und damit $D[f(x_0=1, h< 10^{-10})] = 0$.

\paragraph{Dilemma:}

\begin{itemize}
\item $h$ zu groß $\to$ Diskretisierungsfehler
\item $h$ zu klein $\to$ Rundungsfehler
\end{itemize}

\paragraph{"`etwas"' genauere Analyse}

Gesamtfehler retzt sich zusammen aus Rundungs- \& Diskretisierungsfehler:

\begin{align*}
  \Delta &= |\rd(D_1 f(x_0, h)) - f'(x_0)|\\
  &= |\rd(D_1 f(x_0, h)) -D_1 f(x_0, h) + D_1 f(x_0, h) - f'(x_0)|\\
  &\leq |\rd(D_1 f(x_0, h)) -D_1 f(x_0, h)| + |D_1 f(x_0, h) - f'(x_0)|\\
\intertext{das erste Teil}
&= \left|\rd\left( \frac{f(x_0+h) - f(x_0)}{h} \right) - \frac{f(x_0 + h) - f(x_0)}{h}\right|\\
&\approx \frac{|\rd(f(x_0+h)) - \rd(f(x_0)) - f(x_0+h) + f(x_0)|}{h}\\
&\approx \frac{|\rd(f(x_0+h)) - f(x_0+h)| + |\rd(f(x_0))  - f(x_0)|}{h}\\
&\approx \frac{2|\rd(f(x_0+h)) -f(x_0+h)|}{h}\\
&\leq \frac{2\varepsilon_{\max}}{h}|f(x_0)|\\
\intertext{insgesamt}
\Delta &\approx \frac{2\varepsilon_{\max}}{h} |f(x_0)| + \frac{1}{2} h |f''(x_0)|
\end{align*}

Schrittweite $h$ ist Optimal, wenn Gesamtfehler minimal, also

\begin{align*}
\frac{d\Delta}{dh} &= -\frac{2\varepsilon_{m}}{h^2} |     f(x_0) + \frac{1}{2} | f''(x_0)|\\
\frac{d\Delta}{dh} &= 0\\
&\to h^\star = \sqrt{\frac{4\varepsilon_{m} |f(x_0)|}{|f''(x_0)|}}
\end{align*}

Formel ist in der Praxis nicht sehr relevant, da $f''(x)$ nicht bekannt ist! Gibt aber eine Idee von der zu erwartenden Größenordnung!

Zurück zum Beispiel: $f(x) = \sin(x)$, $f''(x) = -\sin(x)$

\[
x_0 = 1;\quad h^\star = \sqrt{4\varepsilon_{\max}} = \sqrt{5\cdot 5\cdot 10^{-10}} \approx 4.5\cdot 10^{-5}
\]

(Machinenegenauigkeit $\varepsilon_{\max} = 5 \cdot 10^{-10}$) stimmt ungefähr!

\subsubsection*{Iterative Verbesserung von Differenzenformeln}

Bsp: Verbesserung der Differnzenformel 1. Ordnung.

\begin{align*}
  D_1f(x_0,h) &= \frac{f(x_0+h) - f(x_0)}{h}
\intertext{Trick: verwende Schrittweiten $h$ und $\frac{h}{2}$}
 D_1f(x_0,h) - f'(x_0) &= \frac{f(x_0+h) - f(x_0)}{h} - f'(x_0)\\
 &= \frac{1}{2}f''(x_0)h+\frac{1}{6}f'''(x_0)h^2 + \frac{1}{24} f^{iv}(x_0)h^3\\
D_1f(x_0,\frac{h}{2}) - f'(x_0) &= \frac{1}{2} f''(x_0)\frac{h}{2} + \frac{1}{6}f'''(x_0) \frac{h^2}{4} + \frac{1}{24} f^{iv}(x_0) \frac{h^3}{8}\\
&= \frac{1}{4} f''(x_0)h + \frac{1}{24}f'''(x_0) h^2 + \frac{1}{192} f^{iv}(x_0) h^3
\end{align*}
\begin{align*}
  \to 2[D_1f(x_0, \frac{h}{2}) - f'(x_0)] - [D_1f(x_0,h) -f'(x_0)] &\equiv D_1^\star f(x_0,h) - f'(x_0)\\
&= -\frac{1}{12}f'''(x_0) h^2 - \frac{1}{32} f^{iv}(x_0) + \dots\\
D_1^\star f(x_0,h) &= 2D_1f(x_0, \frac{1}{2}) - D_1f(x_0,h)
\end{align*}

ist Differnzenfomrel mit Fehlerordnung 2 \underline{explizit}

\begin{align*}
  D_1^\star f(x_0,h) &= \frac{2(f(x_0+\frac{h}{2}) - f(x_0))}{\frac{h}{2}} - \frac{f(x_0+h) - f(x_0)}{h}\\
&= \frac{4f(x_0+\frac{h}{2}) - 3f(x_0) - f(x_0 + h)}{h}
\end{align*}

\paragraph{allgem. Satz}

Sei $D(h)$ ein Formel zur Näherung von $\overline{D}$ mit der Fehlerentwicklung $D(h) = \overline{D} + C_1h + C_2h^2 = C_3h^3 + \dots$ dann hat die Formel $D^\star(h) = 2D(\frac{h}{2}) - D(h)$ und die Fehlerentwicklung $D^\star (h) - \overline{D} = -\frac{C_2}{2}h^2$

Iterative Fortsetzung möglich $\to$ Formel beliebiger Fehlerordnung $k$ können berechnet werden, nicht auf Differenzenformeln beschränkt! Einzige Voraussetzung: Fehlerentwicklung nach Potenzen einer (kleinen) Größe existiert!

\paragraph{Noch allgemeinerer Satz} Sei $D(h)$ eine Formel der Fehlerordnung $k$ zur Näherung von $\overline{D}$ mit Fehlerentwicklung $D(h) - \overline{D} = C_1h^k + C_2h^{k+1} + C_1h^{k+1}+\dots \to$ extrapolierte Formel: 

\begin{align*}
  D^\star (h) &= \frac{2^k D(\frac{h}{2}) - D(h)}{2^k - 1}\\
  &= D\left(\frac{h}{2}\right) + \frac{D\left(\frac{h}{2}\right) - D(h)}{2^k -1}
\intertext{mit Fehlerentwicklung}
D^\star(h) - \overline{D} &= \frac{-C_2h^{k+1}}{2(2^k - 1)}
\end{align*}

Dies führt zu einem \paragraph{$h$-Extrapolations-Algorithmus}

Sei $D(h)$ eine Formel zur Näherung von $\overline{D}$ mit $D(h) - \overline{D} = C_1h + C_2h^2  + \dots$ definiere $D_{i,0} = D\left(\frac{h}{2^i}\right)$ für $i=0,1,2,\dots \to D_{i,k} = D_{i+1,k-1} + \frac{D_{i+1,k-1} - D_{i,k-1}}{2^k -1}$ für $k=1,2,\dots n$, $i=0,1,\dots n-k$ sind Näherungen für $\overline{D}$ mit Fehlerberechnung $k+1$.

\section{Numerische Integration}

Gesucht ist eine Näherung von $I = \int\limits_a^b f(x)\, dx;\, f(x)\colon [a,b] \to \R$ numerischer Näherungsverfahren $\hat{=}$ "`Quadraturverfahren"'. Teile Integrationsintervall $[a,b]$ in $n$ Teile auf mit $h=\frac{(b-a)}{n}$ und $x_i = a + h\cdot i$ mit $i=0\dots n$.

\begin{itemize}
\item Rechtechregel: $RI(h) = h\sum\limits_{i=0}^{n-1}   f\left(x_i+\frac{h}{2}\right)$

\item Trapezregel: $TI(h) = h\left[\frac{f(a) + f(b)}{2} + \sum\limits_{i=1}^{n-1} f(x_i)\right]$
\item Simpsonregel: $SI(h) = \frac{h}{3}(f(b) + f(a) + 4f(a+h))$
\item Interpolatorische Quadraturformel: Idee: integriere statt $f$ ein Interpolationspolynom
\begin{itemize}
\item Polynom 0. Ordnung $\to$ Rechtechregel
\item Polynom 1. Ordnung $\to$ Trapezregel
\item Polynom 2. Ordnung $\to$ Simpsonregel
  \begin{align*}
    p(x) = f(a) &+ \frac{f(a + h) - f(a)}{h}(x-a)\\
    &+ \frac{\frac{f(b) - f(a+h)}{h} - \frac{f(a+h)-f(a)}{h}}{2h}\cdot(x-a)(x-a+h)
  \end{align*}
(Newtonsche Form des Interpolationspolynoms)


\begin{align*}
  \int_a^b (x-a)\, dx &= \left.\frac{1}{2} (x-a)^2 \right|_a^b =   \frac{1}{2} (b-a)^2 = 2h^2\\
\int_a^b (x-a)(x-a+h)\, dx &= \dots = \frac{2}{3}h^3\\
\end{align*}
\begin{align*}
  \to SI = \int_b^a p(x)\, dx &= f(a)2h + \frac{f(a+h) -f(a)}{h}\cdot   2h^2\\
&+ \frac{f(b)+f(a)-2f(a+h)}{2h^2}\cdot\frac{2}{3}h^3\\
&= \frac{h}{3}\left[ \cancel{6f(a)} + 6(f(a+h) - \cancel{f(a)} + (f(b) - 2f(a+h)+f(a)) \right]\\
SI &= \frac{h}{3}\left[ f(b) + f(a) + 4f(a+h) \right]
\end{align*}
\end{itemize}
\end{itemize}

\section*{Quadraturfehler}

Eine Quadraturformel QI zur Näherung von $I = \int\limits_a^b f(x)\, d(x)$ hat die Fehlerordnung $k$ wenn füra lle Polynome vom Grad $k-1$ gilt: Fehler $E = I-QI = 0$ und mindestens ein Polynom vom Grad $k$ gilt $E \neq 0$

\paragraph{Bspl}

$p(x) = a_0 + a_1x+a_2x^2 + a_3x^3 + a_4x^4$

\begin{align*}
  I = \int\limits_0^{2h} dx\, p(x) &= a_0 2h + a_1\frac{1}{2}(2h)^2 +   a_2 \frac{1}{3}(2h)^3 + a_3\frac{1}{4}(2h)^4+a_4\frac{1}{5}(2h)^5\\
 &= a_0 2h + a_12h^2 + a_2 \frac{8}{3} h^3 +a_3 4h^4 + a_4\frac{32}{5}h^5
\end{align*}
\begin{align*}
RI = h\left[ p\left(\frac{h}{2}\right) + p\left(\frac{3h}{2}\right)\right] &= a_0 2h +ha_1 \left(\frac{h}{2} + \frac{3h}{2}\right) + ha_2\left(\frac{h^2}{4} + \frac{9h^2}{4}\right)\\
&= a_02h + a_1 2h^2 + a_2 \frac{5}{2}h^3+\dots & k=2\\
\end{align*}
\begin{align*}
  TI &= h\left(\frac{p(0) + p(2h)}{2} + p(h)\right)\\
  &= a_02h + ha_1\left( \frac{0}{2} + \frac{2h}{2} + h \right) +     ha_2\left( \frac{0}{2} + \frac{4h^2}{2} + h^2 \right)\\
&= a_0 2h + a_1 2h^2 + a_2 3h^3 + \dots & k=2
\end{align*}
\begin{align*}
  SI =& \frac{h}{3} \left[ p(0) + p(2h) + 4p(h) \right] = \frac{h}{3} a_0 6 + \frac{h}{3}a_1(0+2h+4h)+\\
&+ \frac{h}{3}a_2(0 + 4h^2 + 4h^2) + \frac{h}{3}a_3(0+8h^3+4h^3)+\frac{h}{3}a_4(0+16h^4+4h^4)\\
=& a_02h + a_12h^2 + a_2 \frac{8}{3}h^3 +a_34h^4 + a_4\frac{20}{3}h^5 & k=4\ !
\end{align*}

\section*{Fehlerrechnung pro Intervalllänge}

\begin{align*}
  \left|\frac{I-RI}{2h}\right| = \left| \frac{a_2 \frac{8}{2}h^3 -a_2\frac{5}{2}h^3}{2h} \right| = \frac{a_2}{2h} \frac{16-15}{6}h^3 = a_2 \frac{h^2}{12}
\end{align*}

für beliebige Funktionen $f(x)$:

\begin{align*}
  f(x) = f(x_0) + (x-x_0) f'(x_0) = \frac{1}{2}(x-x_0)^2 f''(x_0) + \frac{1}{6}(x-x_0)^3 f'''(x_0) + \frac{1}{24}(x-x_0)^4f^{iv}(x_0)\\
\end{align*}

$a_2 \approx \frac{1}{2} f'' \to$ Feler ER $\approx \frac{1}{24} f''h^2$

\paragraph{Trapezregel}

\begin{align*}
  \left|\frac{I-TI}{2h}\right| \approx \left| \frac{a_2\frac{8}{3}h^3 - a_2 3h^3}{2h} \right| = \frac{a_2}{2h} \cdot \frac{9-8}{3}h^3 \cong a_2\frac{h^2}{6} \simeq \frac{f''h^2}{12}
\end{align*}

\paragraph{Simpsonregel}

\begin{align*}
  \frac{ |I - SI|}{2h} \approx \frac{\left| a_4\frac{32}{5}h^5 - a_4\frac{20}{3}h^5\right|}{2h} = \frac{a_4}{2h} \frac{100-96}{15} h^5 = \frac{a_42h^4}{15} \cong \frac{1}{180} f^{iv} h^4
\end{align*}

also: sowohl Vorfaktoren als auch Potenzen in $h$ der Fehler sind unterschiedlich. Für Funktionen mit dominanten höheren Ableitungen $\to$ Simpsonregel (bei gleichem numerischen Aufwand).

\begin{itemize}
\item systematische Verbesserung der Quadraturformeln mit   $h$-Extrapolations-Algorithmus möglich!
\item im Prinzip Verbesserung auch möglich durch Interpolation mit Polynomen höherer Ordnung, aber: wie bei Interpolation (VL Kap XY) numerische Instabilitäten.
\item besser: Simpson mit kleinem $h$!
\end{itemize}

\section*{Besser: Gauß Formel zur Quadratur}

\paragraph{Idee}

bisher: äquidistante Stützstellen, kann höhere Fehlerordnung erreicht werden mit variablem Abstand zwischen Stützstellen?

Wir betrachten das Intervall $[-1,1]$, allgemeine Quadraturformel lässt sich schreiben als

\[
Q = \sum_{i=1}^n b_i f(x_i)
\]

mit Gewichten $b_i$ und Stützstellen so gewählt, dass Fehlerordnung $k$ möglichst hoch!

Wähle $n=1$, also $Q = b_1 f(x_1)$, fordere Exaktheit ($Q=I$) für jede Polynomordnung Term für Term.

2 Unbekannte ($b_i$ und $x_i$) also Polynom 1. Ordnung $\left(I = \int\limits_{-1}^1 dx\, p(x)\right)$.

\begin{minipage}{.5\linewidth}
  \begin{tabular}{c|c|cl}
    $p(x)$ & I & Q\\\hline
    1 & 2 & $b_1$ & $\to b_1 = 2$\\
    x & 0 & $b_1x_1$ & $\to x_1 = 0$
  \end{tabular}
\end{minipage}
\begin{minipage}{.5\linewidth}
  eine eindeutige Lösung!
\end{minipage}

Für $n=2$ gibt es 4 Unbekannte, $Q = b_1f(x_1)+b_2f(x_2)$. $I \overset{!}{=} Q$ führt zu nicht-lin.Gl.Syst.

\begin{minipage}{.35\linewidth}
  \begin{tabular}{c|c|c}
    $p(x)$ & I & Q\\\hline
    1 & 2 & $b_1 + b_2$\\
    x & 0 & $b_1x_1 + b_2x_2$\\
    $x^2$ & $\frac{2}{3}$ & $b_1x_1^2 + b_2 x_2^2$\\
    $x^3$ & 0 & $b_1x_1^3 + b_2 x_2^3$
  \end{tabular}
\end{minipage}
\begin{minipage}{.65\linewidth}
  Aus Symmetriegründen probieren wir $\underbrace{x_1 = -x_2}_{\text{Freiheitsgrad eliminiert}}$
\end{minipage}

Aus 2. Gleichung $\to (b_1-b_2)x_1 = 0 \to \underbrace{b_1 = b_2}_{\text{Freiheitsgrad eliminiert}}$. [4. Gleighung $\to (b_1-b_2)x_1^3 = 0 \checkmark$]

1. Gleichung: $b_1 + b_2 = 2 \to b_1 = b_2 = 1$
3. Gleichung: $\frac{2}{3} = b_1x_1^2 + b_2x_2^2 = 2b_1 x_1^2 = 2x_1^2 \to x_1^2 = \frac{1}{3} \to x_{1,2} = \pm \sqrt{\frac{1}{3}}$.

Lösung des Gauß Ansatzes für $n=2$: $b_1 = b_2 = 1$, $x_1 = -\sqrt{\frac{1}{3}},\ x_2 = +\sqrt{\frac{1}{3}}$.

Das allgemeine Gauß Problem ist stets lösbar, die $n$ Stützstellen sind die $n$ Nullstellen des Legendre Polynoms $P_n(x) = \frac{1}{2^n n!}\cdot \frac{d^n}{dx^n} (x^2 - 1)^n$.

Gauß Quadraturformeln für $n=1,2,3$

\begin{itemize}
\item $I = \int\limits_{-1}^1 dx\, f(x)$
\item $n=1$: $G_1I = 2f(0)$, $I = \int$
\item $n=2$: $G_2I = f\left(-\frac{1}{\sqrt{3}}\right) + f\left(\frac{1}{\sqrt{3}}\right)$, $k=4$, vergl\@. RI und TI
\item $n=3$: $G_3I = \frac{5}{9} f(-\sqrt{0.6}) + \frac{8}{9} f(0) + \frac{5}{9}f\left(+\sqrt{0.6}\right)$, $k=6$, vergl\@. SI
\end{itemize}

Fehlerordnung $k=2n$. Vorteil von Gauß Quadratur: Gewichte immer positiv und damit numerisch sehr stabil!

%% break, 20131212

\pagebreak

6.3 partielle DG

Wellengl.

\[
\frac{\partial^2}{\partial t^2} u(x,t) = c^2 \frac{\partial^2}{\partial x^2} u(x,t) \to u(x,t) = f(x \pm ct)
\]

Separationsansatz:

\[
u(x,t) = f(x) \phi(t) \to f(x) \phi''(t) = c^2 \phi(t) f''(x) \to \frac{\phi''(t)}{\phi(t)} = c^2 \frac{f''(x)}{f(x)} = -c^2 k^2 \text{ Konstante}
\]
$\to$ 2 entkoppelte Eigenwertgl.:
\begin{align*}
  \phi''(t) + c^2k^2 \phi(t) &= 0\\
  f''(x) +  k^2 f(x) & = 0
\end{align*}

Federkette: Randbedingung für $F(0),\ f(l)$. Anfangsbedingung in der Zeit.

Separationsansatz funktioniert auch für viele nicht mehr exakt lösbare Probleme! Bsp: inhomogene schwere Federkette.

Homogene Federkette : $\rho \dot{\dot{u}} = T u'' \to \rho(x) \frac{\partial^2}{\partial t^2} u(x,t) = T \frac{\partial^2}{\partial t^2} u(x,t) - \rho(x) g$, $g = 9.8\si{ms^{-2}}$ Erdbeschleinigung, $\rho(x)$ Mamerdichtverteilung.

Lösung der sich anorgenen DG durch Supenparitiion der allgem. homogener Lösung (also für $g=0$) und einer "`partiluklären"' Lösung der sich anorgener DG homogene DG: $u_h(x,t) = f(x) \phi(t) \to f(x)\phi''(t) = \frac{T}{\rho(x)}\phi(t) f''(x) \to \frac{\phi''(t)}{\phi(t)} = \frac{T}{\rho(x)}\frac{f''(x)}{f(x)} = -\omega^2$

$\to$ 2 Gleichungen: $f''(x) + \frac{\omega^2}{T} \rho(x)f(x) = 0$ kann als Eigenwertproblem für beliebige Randwerte $f(0)$, $f(l)$ mit Schießverfahren gelöst werden.

$\phi''(t) + \omega^2 \phi(t) = 0$ kann exakt gelöst werden. Partikuläre Lösung: hier kann statische Lösung benutzt werden: $u_p(x) \to T\frac{\partial^2 u_p(x)}{\partial x^2} -\rho(x)g = 0 \hat{=}$ normales Randwertproblem ($u_p(0)$ und $U_p(l)$) $\to$ allgemeine Lösung $u(x,t) = u_h(x,t) + u_p(x) = f(x)\phi(t) + u_p(x)$. Kann dann wieder aus der Anfangsbedingungen $u(x,t=0)$ konstruiert werden!

Partielle DG mit zeitlich veränderlicher Inhomogenität $\to$ Green's Funktionen. Bspl: Diffusionsgleich.

Partikeldiffusion: Partikel diffundieren und werden erzeugt/vernichtet (Wärmediffusion, chemische Reaktions-Diffusionssysteme, Migration von Zellen und Lebewesen\dots)

$n_i(t)$: Partikelzahl in Zelle $i$ zur Zeit $t$.

\paragraph{Diffusion}

in einem Zeitschritt $\delta t$ macht ein Partikel mit Wahrscheinlichkeit $p$ einen Schritt weg vom Gitterplatz $i$ nach Platz $i+1$ oder $i-1$.

\paragraph{Partikelerhanltung bei Diffusion}

\begin{align*}
  n_i(t + \delta t) &= n_i(t) (1-p) + \frac{p}{2} n_{i+1}(t) +   \frac{p}{2} n_{i-1}(t) + S_i(t)\\
n_i(t + \delta t) - n_i(t) &= \frac{p}{2} \left( n_{i+1}(t) + n_{i-1}(t) -2n_i(t)\right) + S_i(t)\\
\frac{n_i(t + \delta t) - n_i(t)}{\delta t} &= \frac{pa^2}{2\delta t} \left( \frac{n_{i+1}(t) -2n_i(t) + n_{i-1}(t)}{a^2} \right)
\end{align*}

Kontinuumslimes $\delta t \to 0$, $a \to 0$.

\begin{align*}
  \frac{\partial n(x,t)}{\partial t} = D\frac{\partial^2 n(x,t)}{\partial x^2} + S(x,t)
\end{align*}

Diffusionsgleiching in 1D.

Ort $x = ia$, $n(x,t)$: Partikeldichte, $D = \frac{pa^2}{2\delta t}$ Diffusionskonstante, $s(x,t) = \frac{S_i(t)}{\delta t}$: Produktionsrate

In 3D: $\dot{n} = D\Delta n + S$, $\Delta = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}$

Zunächst Anfangswert $n(r, t=0)=0$.

\paragraph{Impuls oder Green's Methode}

Quelltext $S(r,t)$ wird in kurze Impulse zerhackt:

\[
  S(r,\tau) \delta(\tau -t)
\]

und Lösung des Problems als Konvolution über die separaten Lösungen ($\hat{=}$ Green's Funktionen) geschrieben

\begin{align*}
  n(r,t) = \int\limits_0^\infty d\tau \eta(r,t,\tau)
\end{align*}

$\eta$ folgt aus DG
\begin{align*}
  \frac{\partial \eta(n,\tau,t)}{\partial t} = D\Delta\eta + S(r,\tau)\delta(t-\tau)
\end{align*}

Integral über $\tau$ ergibt die originelle PD:

\begin{align*}
  \int\limits_0^\infty \! d\tau \frac{\partial \eta(n,\tau,t)}{\partial t} &= \int\limits_0^\infty\! d\tau\, D\Delta\eta(r,\tau,t) + \int\limits_0^\infty\! d\tau\, S(r,\tau)\delta(t-\tau)\\
\frac{\partial}{\partial t} \int\limits_0^\infty \! d\tau \eta(n,\tau,t) &= D\Delta n(r,t) + S(r,t)\\
\frac{\partial}{\partial t} n(r,t) &= D\Delta n(r,t) + S(r,t)
\end{align*}

$\delta$ ist $\int f(x') \delta(x-x')dx' = F(x)$

Wir machen den Ansatz $\eta(r,\tau,t)\Theta(t-\tau)$

$\Theta(t-\tau)$: Heargride oder Stufenfunktion $\Theta(x)
\begin{cases}
  1 & x > 0\\
  0 & x < 0
\end{cases}$. $\Theta'(x) = d(x)$ da $\int\limits_{-\infty}^x \! dy \delta(y) = \Theta(x)$.

Ansatz einsetzen in PDG:
\begin{align*}
  \text{l.S. } \frac{\partial}{\partial t} \eta(r,\tau,t)\Theta(t-\tau) &= \Theta(t-\tau) \frac{\partial}{\partial t} \eta(r,\tau,t) + \eta(r,\tau,t)\delta(t-\tau)\\
\text{r.S.} &= \Theta(t-\tau) D\Delta\eta(r,\tau,t) + S(r,\tau)\delta(t-\tau)d
\end{align*}

Gleichung wird also gelöst wenn:
\begin{enumerate}
\item $\frac{\partial}{\partial t}\eta(r,\tau,t) = D\Delta\eta(r,\tau,t)$ homogene PDG
\item $\eta(r,\tau,\tau) = S(r,\tau)$ Anfangswertbedingung
\end{enumerate}

Wir haben das inhomogene Problem für $n(r,t)$ (mit Anfangswert $n(r,0)=0$) in das homogene Problem mit Angangswertbedingung $\eta(r,\tau,\tau) = S(r,\tau)$ umgewandelt $\to$ lösbar.


\end{document}
