\documentclass[a4paper,ngerman]{scrbook}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[ngerman]{babel}
\usepackage{hyperref}
\usepackage{multirow}

%Compiler
\usepackage{ifxetex}
\usepackage{ifluatex}
\ifxetex
  \usepackage{fontspec,xunicode}
  \catcode`\ß=13
  \defß{\ss}
\else\ifluatex
  \usepackage{fontspec,xunicode}
\else
  \usepackage[utf8]{inputenc}
\fi\fi
% /Compiler

\newcommand{\bO}{\ensuremath{\mathcal{O}}}%
\newcommand{\R}{\ensuremath{\mathbb{R}}}%

\title{Computergestützte Methoden der exakten Naturwissenschaften }
\subtitle{Computerphysik}
\date{Wintersemester 2013/2014}
\author{Carlos Martín Nieto}
\begin{document}
\maketitle
\chapter{Fehler}

Ziel der Naturwissenschaften: Beschreibung der natur durch (einfache) Gleichungen und deren Lösungen

\paragraph{Problem}

Gleichungen typischerweise nicht lösbar mit Papier und Bleistift.
\begin{itemize}
\item[Lösung 1] Gleichungen vereinfachen $\hat{=}$ Näherung/Approximation
\item[Lösung 2] Numerische Lösung von Gleichungen $\implies$ \boxed{\text{diese Vorlesung}}
\end{itemize}

in Naturwissenschaften wichtig: Genauigkeit (den Fehler) von numerischen Rechenergebnissen beurteilen! Es gibt verschiedene Fehlerquellen
\begin{enumerate}
\item Eingabefehler durch Ungenauigkeiten in den Eingabedaten.
\item Näherungsfehler wenn statt exakten mathematische Ausdrücke vereinfachte Ausdrücke benutzt werden.
\item Modelfehler durch vereinfachte physikalische Modelle.
\item Rundungsfehler durch die numerische Darstellung von Zahlen und der damit verbundenden endlichen Genauigkeiten.
\end{enumerate}

\section{Näherungsfehler}

Viele mathemiatische Ausdrúcke, die in der Physik auftreten sind in der exakten Formulierung nicht oder sehr aufwendig zu berechnen $\to$ Approximation $\to$ Näherungsfehler. Häufig: Funktionen definiert durch unendliche Reihen.

\paragraph{Bsp 1}
Exponenzialfunktionen

Die Funktion ist $e^x = \displaystyle\sum^\infty_{n=0} \frac{x^n}{n!}$. Die Näherung $e^x = \displaystyle\sum^N_{n=0} \frac{x^n}{n!}$

$\displaystyle\frac{df(x)}{dx} = F(x)$ werden durch e-Funktion gelöst.

\paragraph{Bsp 2}
Lösung einer Differenzialgleichung im Kontinuum wird ersetzt durch Lösung der diskretisierten Gleichung.

Ausgangs-DG: $\displaystyle\frac{d}{dx}f(x) = a\cdot f(x)$, Lösung $f(x) = e^{ax}$.

Lösung durch Beschränkung auf diskrete Gitterpunkte. $x_i$ mit $x_{i+1} - x_i = \Delta x \to$

\[
f(x) = \frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i} = a\cdot \frac{f(x_{i+1}) - f(x_i)}{2}
\]

Verbesserung der Näherung durch Verfeinerung der Diskretisierung, also $\Delta x \to 0$.

Nachteil: mehr Rechenoperationen und damit mehr Rechenzeit, außerdem mehr Rundingsfehler (mehr in \autoref{sec:rundungsfehler}).

Ziel der Numerik: optimaler Kompromiss zwischen Fehler und Rechenzeit finden.
\section{Modellfehler}
\label{sec:modellfehler}

\paragraph{Beispiel}
Planetenbewegung

Erster Keppler Gesetzt: Planeten bewegen sich auf elyptischen Bahnen, in einem Brennpunkt steht die Sonne.

Neutonische Bewegungsgleichung
\[
\vec{F} = m\cdot \vec{a} = m\cdot \frac{d^2\vec{r}(t)}{dt^2} = -G \frac{Mm\vec{r}}{|r|^3}
\]

\paragraph{Modelnäherungen}

\begin{enumerate}
\item Sonnenmass $M \gg$ Planetenmasse $m$ (leicht korrigierbar)
\item Reibungskraft $\vec{F}_R = -\gamma \frac{d\vec{r}(t)}{dt}$ vernachlässigt. OK für Planeten, wichtig für kleine Objekte.
\item Gravitationsgesetzt in der einfachen Form gilt nur für Kugeln!
\item Relativistische Effekte $\to$ Merkur-Perikeldrehung.
\item Mehrkörperproblem $r_i(t)$, $i=1,\dots,N$
  \[
  m_i \frac{d^2r_i(t)}{dt^2} = \vec{F_i} = -\sum_{j\neq i} G m_i m_j \frac{\vec{r_i}(t) - \vec{r_j}(t)}{|v_i(t) - r_j(t)|^3}
  \]
\end{enumerate}

Gleichung für $N>2$ wechselwirkende Massen kann leicht hingeschrieben werden (durch Summatia der Knüpfe). Lösung aber nur nummerisch möglich! Annahme: $N=3$ Dreikörperproblem als 3-Stöße!

\chapter{Blah blah}

\chapter{Lineare Gleichungssyteme}
\section{Gauß}

Kompliziertes Zeug.

\section{LR-Zerlegung (LU in en)}

Möchte man mehrere LGS mit diasselben $A$ aber anderere rechter Seite lösen so empfiehlt es sich die Elementaren Umforomungen zu merken.

Für $A =
\begin{pmatrix}
  1 & 2 & 3\\
  6 & -2 & 2\\
  2 & 1 & -4
\end{pmatrix}
$ führen wir durch

\begin{align*}
z_2 = z_2 - 6z_1 &\to L_1 =
\begin{pmatrix}
  1 & 0 & 0\\
  -6 & 1 & 0\\
  0 & 0 & 0
\end{pmatrix}\\
  z_3 = z_3 + 3z_1 &\to L_2 =
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix}\\
  z_3 = z_3 + \frac{1}{2}z_2 &\to L_3 =
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}\\
L_1A &=
\begin{pmatrix}
  1 & 2 & 3\\
  0 & -14 & -16\\
  -3 & 1 & -4
\end{pmatrix}
\end{align*}

\begin{align*}
  L_2L_1 &=
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & 0 & 1
  \end{pmatrix} =
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix}\\
L &= L3(L_1\cdot L_1) = 
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    3 & 0 & 1
  \end{pmatrix} =
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}\\
  LA &= L_3L_2L_1A - R\\
  R &= LA =
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 2 & 3\\
    6 & -2 & 2\\
    2 & 1 & -4
  \end{pmatrix} =
  \begin{pmatrix}
    1 & 2 & 3\\
    0 & -14 & -16\\
    0 & 0 & -3
  \end{pmatrix}
\end{align*}

Dies ist die rechts-obere Dreiechsmatrix aus der letzten VL.

\paragraph{Bsp}

Lösung von $A\vec{x} = \vec{c}$ mit $\vec{c} =
\begin{pmatrix}
  12\\ -16\\ 2
\end{pmatrix}$

\begin{align*}
  L\vec{c} &=
  \begin{pmatrix}
    1 & 0 & 0\\
    -6 & 1 & 0\\
    0 & \frac{1}{2} & 1
  \end{pmatrix}
  \begin{pmatrix}
    12\\ -16\\ 2
  \end{pmatrix}
  =
  \begin{pmatrix}
    12 \\ -88\\ -6
  \end{pmatrix}
\end{align*}

D.h\@. wir losen $R\vec{x} = y\cdot
\begin{pmatrix}
  1 & 2 & 3 & 2\\
  0 & -14 & -16 & -88 \\
  0 & 0 & -3 & -6
\end{pmatrix}$. Rückeinsetzen
\begin{align*}
  x_3 &= \frac{-6}{-3} = 2\\
  14x_2 &= 88 - 16x_3\\
  &= 88 - 32 = 56\\
  \implies x_2 &= 4\\
  x_1 &= 12 - 2x_2 - 3x_3\\
  &= 12 - 8 - 6 = -2\\
  \implies \vec{x} =
  \begin{pmatrix}
    -2\\ 4\\ 2
  \end{pmatrix}
\end{align*}

Satz. Für jede $n \times n$-Matrix, für die der Gauß-Algorithmus durchführbar ohne Zeilenaustauschung durchführbar ist, gibt es $n\times n$-Matrizen $L$ und $R$ mit der Eigenschaften:
\begin{itemize}
\item $L$ ist eine links-untere Dreiechsmatrix mit $l_{ii} = 1$ für $i=1,\dots,n$
\item $R$ ist eine rechts-obere Dreiechsmatrix mit $r_{ii} \neq 0$ für $i=1,\dots,n$.
\item $A = L^{-1}R$ bezeichnet man als \underline{$LR$-Zerlegung} von $A$.
\end{itemize}

Es gilt: \framebox{$A\vec{x} = \vec{b} \iff L\vec{b} = y$ und $R\vec{x} = \vec{y}$}

\section{Cholesky-Zerlegung}
\label{sec:chorlesky}

\paragraph{Definition}

Eine Symmetrische $n \times n$-Matrix $A$ heißt \underline{positiv-definit}, wenn für alle $\vec{x}~\in~R^n$, $\vec{x} = 0$ gilt: $\vec{x}^{T} A \vec{x} > 0$.

\paragraph{Satz}

Für jede positiv-definierte Matrix $A$ gibe es genau eine rechts-obere Dreiechsform mit $r_{ii} > 0$ für $i=1,\dots,n$ und $A=R^{T}R$. Diese Zerlegung heißt Cholesky-Zerlegung.

Zum berechnen der Cholesky-Zerlegung geht man wie folgt vor (Pseudo-Code)


\begin{verbatim}
for i = 1,...n,
  s = a_ii - \sum_{k=1}^{i-1} r_{ki}^2 // für i=1 ist s=a_11
  if s \leq 0
    stop // $A$ ist nicht positiv definit
  else
    r_ii = \sqrt{s}
    for j = i+1,...,n // nicht-Diagonalelemente
      r_{ij} = \frac{1}{r_ii} (a_ij - \sum_{k=1}^{i-1} r_{ki}r_{kj})
    endfor
endfor
\end{verbatim}

\paragraph{Bsp}

$A =
\begin{pmatrix}
  4 & 4 & 2\\
  4 & 5 & 5\\
  2 & 5 & 26
\end{pmatrix}
$
$
  R =
  \begin{pmatrix}
    r_{11} & r_{12} & r_{13}\\
    0 & r_{22} & r_{23}\\
    0 & 0 & r_{33}
  \end{pmatrix} =
  \begin{pmatrix}
    2 & 2 & 1\\
    0 & 1 & 3\\
    0 & 0 & 4
  \end{pmatrix}
$
\begin{itemize}
\item $i=1$

    $s = a_{11} = 4 > 0 \implies r_{11} = \sqrt{4} = 2$
    \begin{itemize}
    \item $j=2$

      $r_{12} = \frac{1}{2}(4) = 2$
    \item $j=3$
      $r_{13} = \frac{1}{2}(2 - 0) = 1$
    \end{itemize}
  \item $i=2$

    $s = a_{22} - \sum_{k=1}^{i-1} r_{ki}^2 = 5 - 2^2 =1 > 0 \implies r_{22} = \sqrt{1} = 1$
\begin{itemize}
\item $j=3$

  $r_{23} = \frac{1}{1} (5 - \sum_{k=1}^1 r_{12} r_{12}) = 5 - 2 = 3$
\end{itemize}
\item $i=3$

  $s = a_{33} = a_{33} - \sum_{k=1}^{2} r_{k3}^2 = 26 - (1^2 + 3^2) = 16 > 0 \implies r_{33} = \sqrt{16} = 4$
\end{itemize}

Test:

\begin{align*}
R^TR &=
\begin{pmatrix}
  2 & 0 & 0\\
  2 & 1 & 0\\
  1 & 3 & 4
\end{pmatrix}
\begin{pmatrix}
  2 & 2 & 1\\
  0 & 1 & 3\\
  0 & 0 & 4
\end{pmatrix} =
\begin{pmatrix}
  4 & 4 & 2\\
  4 & 5 & 5\\
  2 & 5 & 26
\end{pmatrix} = A\\
\intertext{aber:}
RR^T &= 
\begin{pmatrix}
  2 & 2 & 1\\
  0 & 1 & 3\\
  0 & 0 & 4
\end{pmatrix}
\begin{pmatrix}
  2 & 0 & 0\\
  2 & 1 & 0\\
  1 & 3 & 4
\end{pmatrix} =
\begin{pmatrix}
  9 & 5 & 4\\
  5 & 10 & 12\\
  4 & 12 & 16
\end{pmatrix} \neq A
\end{align*}

\paragraph{Bemerkung}

Der numerische Aufwand des Cholesky-Verfahrens beträgt $\left(\frac{1}{6} n^3 + \frac{1}{2}n^2 - \frac{2}{3} n\right)$ Punktoperationen und $n$ Wurzelberechnungen.
Das Gauß-Verfahren benötigt $\left(\frac{n^3}{3} + \frac{n}{3}\right)$ Operationen, also für $n \geq 2$ ist Cholesky schenller!

\section{Fehlerrechnung bei LGS}
\label{sec:fehlerlgs}

Wir wollen untersuchen, wie sich Fehler in einem LGS auf dessen Lösung auswirken. Dazu benötigen wir ein Maß.

\paragraph{Definition}

Eine Abbildung  $\| \cdot \|$ $\R^n \to \R$ heißt \underline{Vektornorm}, wenn für alle $\vec{x}, \vec{y} \in R^n$ und alle $\lambda \in \R$ gilt:
\begin{itemize}
\item $\|\vec{x}\| \geq 0$ und $\|\vec{x}\| = 0$ wenn $\vec{x} = 0$.
\item $\|\lambda x\| \geq 0$ und $\|\vec{x}\| = 0$ wenn $\vec{x} = 0$.
\item $\|\vec{x} + \vec{y}\| \leq \|\vec{x}\| + \|\vec{y}\|$
\end{itemize}

Beispiele sind

\begin{itemize}
\item Summenorm (1-Norm) $\|\vec{x}\| = \sum_{i=1}^n |x_i|$
  \item Maximumnorm ($\infty$-Norm) $\|\vec{x}\|_{\infty} = \max_{i=1,\dots,n} |x_i|$
\item euklidische Norm (2-Norm) $\|\vec{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}$
\end{itemize}

Alle Vektornormen sind äquivalent, d.h\@. es existieren $c_1,c_2 \in \R$ so dass

\[
c_1\|\vec{x}\|_a \leq \|\vec{x}\|_b \leq c_2 \|\vec{x}\|_a
\]

D.h\@. falls es eine Vektorfolge in einer Norm $a$ konvergiert, so konvergiert sie auch inder Norm $b$. Man kann entsprechende Matrixnomren definieren für eine $A = (a_{ij})$.
\begin{itemize}
\item Spaltensumme (1-Norm) $\|A\|_1 = \max_{j=1,\dots,n} \sum_{i=1}^n |a_{ij}|$
\item Spektralnorm (2-Norm) $\|A\|_2 = \sqrt{S(A^TA)}$, mit $S(A) = \max\{\lambda\}$ "`Spektralradius von $A$"', $\lambda\colon$ Eigenwerte von $A$.
\item Zeilensummennorm ($\infty$-Norm) $\|A\|_\infty = \max\limits_{i=1,\dots,n} \sum_{i=1}^n |a_{ij}|$
\end{itemize}

\paragraph{Bemerkung}

Matrixnormen erfüllen alle Eigenschaften der zugrundelegenden Vektornormen. Insbesondere auch die Äquivalentz

\[
\|A\vec{x}\|_v \leq \|A\|_v\|\vec{x}|_v
\]

Dann sagt man die Norm ist kompatibel

\paragraph{Satz}

Sei $\|\cdot\|$ eine Norm, $A$ eine reguläre $n\times n$-Matrix die Vektoren $\vec{x}, \vec{x'}, \vec{b}, \vec{b'} \in \R^n$ so dass $A\vec{x} = \vec{b} und A\vec{x'} = \vec{b'}$.

Dann gilt

\begin{align*}
  A\cdot (\vec{x}\vec{x'}) &= \vec{b} - \vec{b'}\\
  \vec{x} - \vec{x'} &= A^{-1}(\vec{b} - \vec{b'})\\
  \|\vec{x}-\vec{x'}\| \leq \|A^{-1}\| \|\vec{b} - \vec{b'}\|
\end{align*}

und entsprechend nach Multiplikation mit
\begin{align}
  \|A\|\|\vec{x}\| \geq \|\vec{b}\| \to   \|\frac{1}{\vec{x}}\| \leq     \frac{\|A\|}{\|\vec{b}\|}\\
  \frac{\|\vec{x} - \vec{x'}\|}{\|\vec{x}\|} \leq \|A\| \|A^{-1}\| \cdot \frac{\|\vec{b}-\vec{b'}\|}{\|\vec{b}\|}
\end{align}

Dann nennt $\|A\| \|A^{-1}\| = \text{cond}(A)$ die \underline{Konditionszahl} der Matrix $A$ bzgl\@. der verwendeten Norm.

\paragraph{Bemerkung}

Die Konditionszahl gibt also die max\@. Verstärkung des realtiven Fehlers (3.2) an, wahrend $\|A^{-2}\|$ die maximale Verstärkung des absoluten Fehlers ist (3.1).

\paragraph{Beispiel}

$A\vec{x} = \vec{b}$ mit $\vec{A} =
\begin{pmatrix}
  3 & -6 \\ -6 & 8
\end{pmatrix}$ und $ \vec{b} =
\begin{pmatrix}
  18\\ 28
\end{pmatrix}
$

\begin{align*}
  \det A &= 24 - 36 = -12\\
  A^{-1} = \frac{1}{\det A} \cdot \text{adj}(A) = \frac{1}{-12}
  \begin{pmatrix}
    8 & -6\\ -6 & 3
  \end{pmatrix} =
  \begin{pmatrix}
    \frac{-2}{3} & \frac{1}{2}\\ \frac{1}{2} & \frac{-1}{4}
  \end{pmatrix}\\
  \|A\|_1 &= \max\{9,14\} = 14 = \|A\|_\infty\\
  \|A^{-1}\| & =\max\left\{\frac{7}{6}, \frac{3}{4}\right\} = \frac{7}{6} = \|A^{-1}\|_\infty\\
\|\vec{b}\|_1 &= 46 & \text{cond} A = \|A\|_1 \|A^{-1}\|
\end{align*}

Wir nehmen an $\|\vec{b}-\vec{b'}\| \leq 0.1$.

Abschätzung absoluter Fehler: $|\vec{x}\vec{x'}| \leq \|A^{-1}\| \|\vec{b} - \vec{b'}\| \leq \frac{7}{6}\cdot 0.1 = 0.11666\dots$.

Relativer Fehler $\frac{\|\vec{x}-\vec{x'}\|}{\|\vec{x}\|} \leq \text{cond} A \frac{\|\vec{b}-\vec{b'}}{\|\vec{b}\|} \leq 16.333\dots  \cdot \frac{0.1}{16} \approx 3.35\%$

Der relativer Fehler hängt ab von cond($A$) und $\frac{1}{\|\vec{b}\|}$!

\paragraph{Satz}

Ist nicht nur die rechte Seite eines LGS fehlerbehaftet, sonder auch die Koeffizientenmatrix, so gilt fúr die Lösung der beiden LGS $A\vec{x} = \vec{b}$ und $A'\vec{x'} = \vec{b'}$ mit $\Delta A = A- A'$ und $\Delta \vec{x} = \vec{x} - \vec{x'}$, $\Delta\vec{b} = \vec{b}-\vec{b'}$

\[
\frac{\|\Delta\vec{x}\|}{\|\Delta\vec{x}\|} \leq \frac{\text{cond}(A)}{1-\text{cond}(A) \frac{\|\Delta A\|}{\|A\|}} \left(\frac{\|\Delta A\|}{\|A\|} + \frac{\|\Delta \vec{b} \|}{\|\vec{b}\|}\right)
\]

\section{Iterative Verfarhen}
\label{sec:iterative-verfahren}

Idee: LGS $Ax = b$ ist äquivalent zu einem vektoriellen Nullstellenproblem $Ax - b = \vec{0}$. Anstelle des Nullstellenrproblems betrachtet man das Fixpunktproblem!

Zerlegung der Matrix $A = I + A - I$ ($I$ ist die Einheitsmatrix). Aus $Ax - b = \vec{0}$ wird


\begin{align*}
  \vec{0} &= (I + A - I)x - b \to Ix = (I - A)x + b\\
  x &= (I-A)x + b
\end{align*}

vektorielle Fixpunktgleichung, kann als Iterationsgleichung interpretiert werden. Startwert $x^{(0)} \to x^{(1)} = (I-A)x^{(0)} + b$ und allgemein

\[
x^{(n+1)} = (I-A)x^{(n)} + b
\]

Fixpunkt der Iterationsgleichung $\hat{=}$ Lösung des LGS.

\paragraph{Beispiel}

$A =
\begin{pmatrix}
  4 & -1 & 1\\
  -2 & 5 & 1\\
  1 & -1 & 5
\end{pmatrix}
$, $b =
\begin{pmatrix}
  5\\ 11\\ 12
\end{pmatrix}
$


\begin{align*}
  x^{(n+1)} &= \left[
    \begin{pmatrix}
      1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1
    \end{pmatrix}
    -
    \begin{pmatrix}
      4 & -1 & 1\\ -2 & 5 & 1\\ 1 & -1 & 5
    \end{pmatrix}
  \right]x^{(n)} +
  \begin{pmatrix}
    5 \\ 11\\ 12
  \end{pmatrix}\\
  x^{(n+1)} &=
  \begin{pmatrix}
    -3 & 1 & -1\\ 2 & -4 & -1\\ -1 & 2 & -4
  \end{pmatrix} x^{(n)} +
  \begin{pmatrix}
    5 \\ 11\\ 12
  \end{pmatrix}
\end{align*}


\begin{center}
  \begin{tabular}{c|c|c|c|c|}
    i & 0 & 1 & 2 & 3\\\hline
    \multirow{3}{*}{$x^{(i)}$} & 0 & 5 & -11 & 22\\\cline{2-5}
    & 0 & 11 & -35 & 148\\\cline{2-5}
    & 0 & 12 & -19 & 29
  \end{tabular}\\
\end{center}
Ansatz nicht konvergent! Betracthe alternive additive Zerlegung $A = A_1 + A_2$

\subsection{Jacobi Verfahren (Gesamtschritt)}
\label{sec:jacobi-gesamt}

Betrachte Zerlegung


\begin{align*}
  A &= L
  \begin{pmatrix}
    0 & 0 & 0 & \cdot\\
    a_{21} & 0 & 0 & \cdot\\
    a_{31} & a_{32} & 0 & \cdot\\
    \cdot & \cdot & \cdot & \cdot
  \end{pmatrix}
  + D
  \begin{pmatrix}
    a_11 & 0 & 0 & \cdot\\
    0 & a_{22} & 0 & \cdot\\
    0 & 0 & a_{33} & \cdot\\
    \cdot &\cdot &\cdot &\cdot
  \end{pmatrix}
  + R
  \begin{pmatrix}
    0 & a_{12} & a_{13} & \cdot\\
    0 & 0 & a_{23} & \cdot\\
    0 & 0 & 0 & \cdot\\
    \cdot &\cdot &\cdot &\cdot
  \end{pmatrix}\\
  Ax-b = 0 &= (L + D + R)x-b \to Dx = -(L+R)x+b\\
  x &= \{\text{FP-Gl}\}-D^{-1}(L+R)x + D^{-1}b \to \{\text{It-Gl}\}x^{(n+1)} = -D(L+R)x^{(n)}  D^{-1} b
\end{align*}

Bsp

\[
A =
\begin{pmatrix}
  4 & -1 & 1\\ -2 & 5 & 1\\ 1 & -2 & 5
\end{pmatrix}
 \to L =
 \begin{pmatrix}
   0  & 0 & 0\\ -2 & 0 & 0\\ 1 & -2 & 0
 \end{pmatrix}
 ,\ D=
 \begin{pmatrix}
   4 & 0 & 0\\ 0 & 5 & 0\\ 0 & 0 & 5
 \end{pmatrix}
 ,\ R=
 \begin{pmatrix}
   0 & -1 & -1\\ 0 & 0 & 1\\ 0 & 0 & 0
 \end{pmatrix}
\]


\begin{align*}
  x^{(n+1)} &= -
  \begin{pmatrix}
    0.25 & 0 & 0\\ 0 & 0.2 & 0\\ 0 & 0 & 0.2
  \end{pmatrix}
  \left[
    \begin{pmatrix}
      0 & -1 & 1\\ -2 & 0 & 1\\ 1 & -2 & 0
    \end{pmatrix} -
    \begin{pmatrix}
      5\\ 11\\ 12
    \end{pmatrix}
  \right]\\
  &=
  \begin{pmatrix}
    0 & 0.25 & -0.25\\ 0.4 & 0 & -0.2\\ -0.2 & 0.4 & 0
  \end{pmatrix} x^{(n)} +
  \begin{pmatrix}
    1.25\\ 2.2\\ 2.4
  \end{pmatrix}
\end{align*}

\begin{tabular}{c|c|c|c|c|c|c|}
  i & 0 & 1 & 2 & 3 & 4 & 5\\\hline
  \multirow{3}{*}{$x^{(i)}$} & 0 & 1.25 & 1.2 & 1.0475 & 1.0065 & 0.9973\\\cline{2-7}
  & 0 & 2.2 & 2.22 & 2.74 & 2.0094 & 1.9986\\\cline{2-7}
  & 0 & 2.4 & 3.03 & 3.048 & 3.0201 & 3.0024
\end{tabular}

\subsection{Einzelschrittverfahren (Gauß-Seidel Verfahren)}
\label{sec:gauss-seidel}

Jakobi-Verfarhen in Komponenten:

%% FIMXE: function numbering per subsection?

\begin{align}
  x_1^{(n+1)} &= 0.25\cdot x_2^{(n)} - 0.25 \cdot x_3^{(n)} + 1.25\\
  x_2^{(n+1)} &= 0.4\cdot x_1^{(n)} - 0.2\cdot x_3^{(n)} + 2.2\\
  x_3^{(n+1)} &= -0.2\cdot x_1^{(n)} + 0.4 \cdot x_2^{(n)} + 2.4
\end{align}

Wenn man annimmt, dass $x^{(n+1)}$ komponentenweise näher am Lösungsvektor liegt, sollte man in Gl (2)(3.4) $x_1^{(n+1)}$ benutzen und in Gl (3)(3.5) $x_1^{(n+1)}$ und $x_2^{(n+1)}$, wie man hierunter sieht.

\begin{align}
  x_1^{(n+1)} &= 0.25\cdot x_2^{(n)} - 0.25 \cdot x_3^{(n)} + 1.25\\
  x_2^{(n+1)} &= 0.4\cdot x_1^{(n+1)} - 0.2\cdot x_3^{(n)} + 2.2\\
  x_3^{(n+1)} &= -0.2\cdot x_1^{(n+1)} + 0.4 \cdot x_2^{(n+1)} + 2.4
\end{align}

In Matrix-Schreibweise

\begin{align*}
  x^{(n+1)} &=
  \begin{pmatrix}
    0 & 0.25 & -0.25\\ 0 & 0 & -0.2\\ 0 & 0 & 0
  \end{pmatrix} x^{(n)} +
  \begin{pmatrix}
    0 & 0 & 0\\ 0.4 & 0 & 0\\ -0.2 & 0.4 & 0
  \end{pmatrix} +
  \begin{pmatrix}
    1.25\\ 2.2\\ 2.4
  \end{pmatrix}
  \intertext{oder:}
  x^{(n+1)} &= D^{-1} \left[ Rx^{(n)} +  Lx^{(n+1)} -b \right]\\
  Dx^{(n+1)} &= -Rx^{(n)} - Lx^{(n+1)} + b & \text{It.-Gl. Gauß-Seidel}\\
  \to (D+L) x^{(n+1)} &= - Rx^{(n)} + b\\
  \to x^{(n+1)} &= -(D+L)^{-1} Rx^{(n)} + (D+L)^{-1}b % box
\end{align*}

\paragraph{In der Praxis}

wird nicht das Inverse der Matrix $(D+L)$ berechnet, sondern das Gl.-Syst\@. $(D+L)x^{(n+1)} = -Rx^{(n)}+ b$ wird gelöst (durch Vorwärtseinsetzen)!

Zurück zum Beispiel

\begin{center}
  \begin{tabular}{c|c|c|c|c|c|}
    & 0 & 1 & 2 & 3 & 4\\\hline
    & 0 & 1.25 & 1.1175 & 1.006 & 1.001\\\hline
    & 0 & 2.7 & 2.001 & 2.007 & 2.0002\\\hline
    & 0 & 3.23 & 2.977 & 3.002 & 2.9998
  \end{tabular}
\end{center}
Konvergiert (etwas) schneller als Gesamtschrittverfahren.

\subsection{Konvergenz von linearen Iterationverfahren}

Gegeben sei eine lineare Matrix-Fixpunkt-Iterationsgleichung $x^{(n+1)} = Bx^{(n)}$ wobei $B$ eine $m \times m$-Matrix, $b \in \R^n$. $\overline{x}$ sei ein Fixpunt mit $\overline{x} = B\overline{x}  b$.

Sei $\|\cdot\|$ ine Matrixnorm (siehe letzte VL), dann gilt: \begin{itemize}
\item[] $\overline{x}$ ist ein anziehenden Fixpunkt, falls $\|B\| < 1$
\item[] $\overline{x}$ ist ein abstoßenden Fixpunkt, falls $\|B\| > 1$
\end{itemize}

Für anziehenden Fixpunkt: Fixpunktiteration konvergiert für \underline{alle} Startwerte.

\paragraph{Bemerkungen}

\begin{itemize}
\item für Gesamtschrittverfahren (Jakobi) gilt $B = -D^{-1}(L+R)$, für die $\infty$-Norm (Zeilensummennorm) gilt (im Hinterkopft behalten, dass $A = D+L+R$):
\begin{align*}
  \|B\|_\infty &= \max\limits_{i=1,m} \sum_{j\neq i} \frac{|a_{ij}|}{|a_ii|} = \max\limits_{i=1,m} \underbrace{\frac{1}{|a_{ii}|}}_{\text{aus $D^{-1}$}} \sum_{j\neq i} \underbrace{|a_{ij}|}_{\text{aus $(L+R)$}} < 1\\
&\iff |a_{ii}| > \sum_{j\neq i} |a_{ij}| \text{ für \underline{alle} $i$}\\
&\hat{=} \text{ "`Zeilensummenkriterium"'}
\end{align*}

So eine Matrix heißt "`diagonaldominant"'

\item Im Einzelschrittverfahren ist $B = -(D+L)^{-1} R$ und man kann zeigen, dass $\| (D+L)^{-1} R \|_\infty \leq \|D^{-1} (L+R)\|_\infty$. \underline{Also}: konvergiert das Gesamtschrittverfahren, so konvergiert auch das Einzelschrittverfahren (Inverse ist nicht immer richtig).
\end{itemize}

\section{Nichtlineare Gleichungsssyteme | Newton Verfahren}
\label{sec:nichtlineare}

Häufig: $n$ nicht-lineare Gleichungen mit $n$ Unbekannten.\\
Gegeben: vektorielle Funktion $f\colon \R^n \to \R^n$.\\
Gesucht: Vektor $\overline{x} \in \R^n$ mit $f(\overline{x}) = \vec{0}$.

\begin{align*}
  f(x) &= f(x_1,\dots,x_n) =
  \begin{pmatrix}
    f_1(x_1,\dots,x_n)\\ f_2(x_1,\dots,x_n)\\ \vdots \\ f_3(x_1,\dots,x_n)
  \end{pmatrix} =
  \begin{pmatrix}
    0\\ 0\\ \vdots\\ 0
  \end{pmatrix}
\end{align*}

Es gibt kein einfaches Verfarhen, um zu prüfen, ob Gl.-Syst\@. lösbar ist! Wie viele Lösungen? $\to$ verallgemenere Newton-Verfahren auf $n$ Dimensionen!

\begin{align*}
  g\colon \R \to \R &\implies g(x) \tilde{=} g(x_0) + (x-x_0)g'(x_0)
  \overset{!}{=} 0\\
  f\colon \R^n \to \R^n &\implies f(x) \tilde{=} f\left(x^{(0)}\right) + D\left[ f\left(x^{(0)}\right)\right] \left(x-x^{(0)}\right)\overset{!}{=} 0
\end{align*}

Lösen des linearen Gl.-Syst\@.

\begin{align*}
  x &= x^{(0)} - \left[ D\left[f\left(x^{(0)}\right)\right]\right]^{-1} f\left(x^{(0)}\right)
\intertext{oder als Iterationsgleichung}
x^{(n+1)} &= x^{(n)} - \left[ D\left[f\left(x^{(n)}\right)\right]\right]^{-1} f\left(x^{(n)}\right)
\end{align*}

Noch mal: nide die inverse einer Matrix berechnen!

\end{document}
